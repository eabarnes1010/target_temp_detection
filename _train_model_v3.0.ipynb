{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a650402-4774-49cb-9b72-9c8f1dd02f1d",
   "metadata": {
    "id": "4a650402-4774-49cb-9b72-9c8f1dd02f1d",
    "tags": []
   },
   "source": [
    "# Detecting temperature targets\n",
    "##### authors: Elizabeth A. Barnes and Noah Diffenbaugh\n",
    "##### date: March 14, 2022 15:14 Mountain Time\n",
    "\n",
    "##### README:\n",
    "\n",
    "* All user-defined parameters (other than plotting colors etc) are defined in the top cell called \"User-defined Experiment Parameters\". Most of your edits should just be there.\n",
    "* The code will train either a typical regression problem ```reg``` or it will predict the conditional normal distribution ```shash2```\n",
    "* The code provides the option of saving the tensorflow models (and their metadata) so that you can load them later and make predictions. This may be particularly useful if you find a good model that you like (and don't want to retrain over and over again) or if you want to save ~50 models and then use them to predict the observational fields to make a distribution of uncertainty (like I did in the google slides)\n",
    "* The random seed calls should be written so that everything is exactly reproducible. So, run the same code tomorrow and you should get identical answers.\n",
    "* Just a note that unfortunately Colaboratory is really slow for the ```shash2``` model (~20+ min!). I am happy to train models for us on my awesome M1 chip laptop when the time comes, but I figured you want to play from start to finish. The good news though is that the parameters I have below are already pretty good ones!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de167a6-1c3f-435f-815c-0ff078ef9d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## User-defined Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454258d-b1e3-4815-952c-f68cd9435c6d",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1646449681009,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "a0d3faaa-4f55-4d7f-a538-6c641098687f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_settings = {\n",
    "    \"exp_name\": 'test',\n",
    "    \"save_model\": True,\n",
    "    \"n_models\": 3,                        # the number of networks you want to train (via a loop)\n",
    "    \"ssp\" : \"370\",                        #[options: '126' or '370']\n",
    "    \"target_temp\": 1.5,\n",
    "    \"n_train_val_test\" : (7,3,0),\n",
    "    \"baseline_yr_bounds\": (1850,1899),\n",
    "    \"training_yr_bounds\": (1970,2100),\n",
    "    \"anomaly_yr_bounds\": (1951,1980),\n",
    "    \"anomalies\": True,                    #[options: True or False]\n",
    "    \"remove_map_mean\": False,             #[options: False or \"weighted\" or \"raw\"]\n",
    "    \"show_plots\" : False,                 #[options: False or True]\n",
    "\n",
    "    \"network_type\": 'reg',                #[options: \"reg\" or \"shash2\"]\n",
    "    \"hiddens\": [10,10],\n",
    "    \"dropout_rate\": 0.,\n",
    "    \"ridge_param\": [1.0,0.0], \n",
    "    \"learning_rate\": 0.0001,              # reg->0.0001, shash2->.00005 or .00001\n",
    "    \"batch_size\": 64,\n",
    "    \"rng_seed\": 8889,\n",
    "    \"act_fun\": [\"relu\",\"relu\"],\n",
    "    \"n_epochs\": 25_000,\n",
    "    \"patience\": 50,\n",
    "    \"verbosity\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fc82c-3e30-4466-b896-1b292d5b2ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc0268-dd0a-4296-8a0b-f14b4c6189b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1646449663380,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "1fdc0268-dd0a-4296-8a0b-f14b4c6189b2",
    "outputId": "4e614eaa-b436-4f26-f1bd-3b59fcad893e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "print('IN_COLAB = ' + str(IN_COLAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb968382-4186-466e-a85b-b00caa5fc9be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17642,
     "status": "ok",
     "timestamp": 1646449680995,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "fb968382-4186-466e-a85b-b00caa5fc9be",
    "outputId": "d7964af9-2d52-4466-902d-9b85faba9a91",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install xarray==0.20.2\n",
    "    !pip install nc-time-axis\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Softmax\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "savefig_dpi = 300\n",
    "np.warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5cee3-6f4f-4818-92e1-1351eeeb565a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1646449681009,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "29a5cee3-6f4f-4818-92e1-1351eeeb565a",
    "outputId": "e5f5b0ac-82b8-4147-bf44-4bc3b49466a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"python version = {sys.version}\")\n",
    "print(f\"numpy version = {np.__version__}\")\n",
    "print(f\"xarray version = {xr.__version__}\")  \n",
    "print(f\"tensorflow version = {tf.__version__}\")  \n",
    "print(f\"tensorflow-probability version = {tfp.__version__}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d089c5-afe9-4d5c-93d1-b67662256c1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eea0ec-ce97-469e-b7d6-da1fa0863927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if exp_settings[\"ssp\"] == '370':\n",
    "    filenames = ('tas_Amon_historical_ssp370_CanESM5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "                 'tas_Amon_historical_ssp370_GISS-E2-1-G_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "                 'tas_Amon_historical_ssp370_IPSL-CM6A-LR_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "                 'tas_Amon_historical_ssp370_UKESM1-0-LL_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "                 'tas_Amon_historical_ssp370_ACCESS-ESM1-5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "                )\n",
    "elif exp_settings[\"ssp\"] == '126':\n",
    "    filenames = ('tas_Amon_historical_ssp126_CanESM5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "                 'tas_Amon_historical_ssp126_MIROC6_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "                 'tas_Amon_historical_ssp126_ACCESS-ESM1-5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "                 'tas_Amon_historical_ssp126_UKESM1-0-LL_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "            )\n",
    "else:\n",
    "    raise NotImplementedError('no such SSP')\n",
    "\n",
    "N_GCMS = len(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9edd00-2ab5-4f44-9bfa-751efe9f749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    MODEL_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/'\n",
    "    FILE_DIRECTORY = MODEL_DIRECTORY\n",
    "    # raise ValueError('@Noah - please set the GoogleDrive model_path above where you want to save things')\n",
    "else:\n",
    "    MODEL_DIRECTORY = 'saved_models/'        \n",
    "    FILE_DIRECTORY = 'saved_files/'\n",
    "    DATA_DIRECTORY = 'data/'\n",
    "    \n",
    "    \n",
    "def save_pred_obs(pred_vector, save_name):\n",
    "    with open(FILE_DIRECTORY + save_name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(pred_vector, f)\n",
    "        \n",
    "\n",
    "def save_tf_model(model, model_name):\n",
    "    \n",
    "    # save the tf model\n",
    "    tf.keras.models.save_model(model, MODEL_DIRECTORY + model_name + \"_model\", overwrite=True)\n",
    "\n",
    "    # save the meta data\n",
    "    save_exp_settings = exp_settings.copy()\n",
    "    save_exp_settings['random_seed'] = int(SEED)\n",
    "    save_exp_settings['train_members'] = train_members.tolist()\n",
    "    save_exp_settings['val_members'] = val_members.tolist()\n",
    "    save_exp_settings['test_members'] = test_members.tolist()\n",
    "    with open(MODEL_DIRECTORY + model_name + '_metadata.json', 'w') as json_file:\n",
    "        json_file.write(json.dumps(save_exp_settings))\n",
    "        \n",
    "    # example code for loading a tf model\n",
    "    # model = tf.keras.models.load_model(MODEL_DIRECTORY + model_name + \"_model\", compile = False)\n",
    "    # predictions = model.predict(x_data)\n",
    "        \n",
    "def get_netcdf_da(filename):\n",
    "    if IN_COLAB:\n",
    "        !pip install wget\n",
    "        import wget\n",
    "        nc_file = wget.download(\"https://eabarnes-data.atmos.colostate.edu/share/\" + filename)\n",
    "    else:\n",
    "        nc_file = DATA_DIRECTORY + filename\n",
    "    \n",
    "    da = xr.open_dataarray(nc_file)\n",
    "    return da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53d22e-95cb-4a2e-86b7-05dd68e5136e",
   "metadata": {
    "id": "ca53d22e-95cb-4a2e-86b7-05dd68e5136e",
    "tags": []
   },
   "source": [
    "### Define the target year, labels and pre-processing\n",
    "\n",
    "For starters, we will define the target year as the year that the ensemble mean, global mean temperature crosses the ```TARGET_TEMP``` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae33333-64de-4bad-86c8-73b31083dacb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "executionInfo": {
     "elapsed": 2814,
     "status": "ok",
     "timestamp": 1646449704589,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "aae33333-64de-4bad-86c8-73b31083dacb",
    "outputId": "195a23ad-e907-478f-d6bf-758b287e3068",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_labels(da, plot=False):\n",
    "    # compute the ensemble mean, global mean temperature\n",
    "    # these computations should be based on the training set only\n",
    "    da_ens = da.mean(axis=0)\n",
    "    weights = np.cos(np.deg2rad(da_ens.lat))\n",
    "    weights.name = \"weights\"\n",
    "    temp_weighted = da_ens.weighted(weights)\n",
    "    global_mean = temp_weighted.mean((\"lon\", \"lat\"))\n",
    "    \n",
    "    global_mean_ens = da.weighted(weights)\n",
    "    global_mean_ens = global_mean_ens.mean((\"lon\",\"lat\"))\n",
    "    \n",
    "    # compute the target year \n",
    "    baseline_mean = global_mean.sel(time=slice(str(exp_settings[\"baseline_yr_bounds\"][0]),str(exp_settings[\"baseline_yr_bounds\"][1]))).mean('time')\n",
    "    iwarmer = np.where(global_mean.values > baseline_mean.values+exp_settings[\"target_temp\"])[0]\n",
    "    target_year = global_mean[\"time\"].values[iwarmer[0]]\n",
    "\n",
    "    # plot the calculation to make sure things make sense\n",
    "    if plot == True:\n",
    "        for ens in np.arange(0,global_mean_ens.shape[0]):\n",
    "            global_mean_ens[ens,:].plot(linewidth=1.0,color=\"gray\",alpha=.5)\n",
    "        global_mean.plot(linewidth=2,label='data',color=\"aqua\")\n",
    "        plt.axhline(y=baseline_mean, color='k', linestyle='-', label='baseline temp')\n",
    "        plt.axhline(y=baseline_mean+exp_settings[\"target_temp\"], color='tab:blue',linewidth=1., linestyle='--', label='target temp')\n",
    "        plt.axvline(x=target_year,color='tab:blue',linewidth=1., linestyle='--', label='target year')\n",
    "        global_mean_obs.plot(linewidth=2,label='data',color=\"tab:orange\")        \n",
    "        plt.xlabel('year')\n",
    "        plt.ylabel('temp (K)')\n",
    "        plt.title(f + '\\ntargets [' + str(target_year.year) + ', ' + str(exp_settings[\"target_temp\"]) + 'C]',\n",
    "                  fontsize = 8,\n",
    "                 )\n",
    "        plt.show()\n",
    "    \n",
    "    # define the labels\n",
    "    print('TARGET_YEAR = ' + str(target_year.year))\n",
    "    labels = target_year.year - da['time.year'].values\n",
    "    \n",
    "    return labels, da['time.year'].values, target_year.year\n",
    "\n",
    "def preprocess_data(da, MEMBERS):\n",
    "\n",
    "    if MEMBERS is None:\n",
    "        new_data = da\n",
    "    else:\n",
    "        new_data = da[MEMBERS,:,:,:]\n",
    "\n",
    "    if exp_settings[\"anomalies\"] is True:\n",
    "        new_data = new_data - new_data.sel(time=slice(str(exp_settings[\"anomaly_yr_bounds\"][0]),str(exp_settings[\"anomaly_yr_bounds\"][1]))).mean('time')\n",
    "        \n",
    "    if exp_settings[\"remove_map_mean\"]  == 'raw':\n",
    "        new_data = new_data - new_data.mean((\"lon\",\"lat\"))\n",
    "    elif exp_settings[\"remove_map_mean\"] == 'weighted':\n",
    "        weights = np.cos(np.deg2rad(new_data.lat))\n",
    "        weights.name = \"weights\"\n",
    "        new_data_weighted = new_data.weighted(weights)\n",
    "        new_data = new_data - new_data_weighted.mean((\"lon\",\"lat\"))\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def make_data_split(da, data, f_labels, f_years, labels, years, MEMBERS):\n",
    "\n",
    "    # process the data, i.e. compute anomalies, subtract the mean, etc.\n",
    "    new_data = preprocess_data(da, MEMBERS)    \n",
    "    \n",
    "    # only train on certain samples\n",
    "    iyears = np.where((f_years >= exp_settings[\"training_yr_bounds\"][0]) & (f_years <= exp_settings[\"training_yr_bounds\"][1]))[0]    \n",
    "    f_years = f_years[iyears]\n",
    "    f_labels = f_labels[iyears]            \n",
    "    new_data = new_data[:,iyears,:,:]\n",
    "    \n",
    "    if data is None:\n",
    "        data = new_data.values\n",
    "        labels = np.tile(f_labels,(len(MEMBERS),1))        \n",
    "        years = np.tile(f_years,(len(MEMBERS),1))\n",
    "    else:\n",
    "        data = np.concatenate((data,new_data.values),axis=0)        \n",
    "        labels = np.concatenate((labels,np.tile(f_labels,(len(MEMBERS),1))),axis=0)        \n",
    "        years = np.concatenate((years,np.tile(f_years,(len(MEMBERS),1))),axis=0)\n",
    "    \n",
    "    return data, labels, years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3a1a6-74cf-44e3-abf6-2088cbea8e73",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485ac17-4b9d-460d-9486-29fb892143f4",
   "metadata": {},
   "source": [
    "#### Load observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ce429-e5d7-4567-9bfe-3b19468fe5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_filename_obs = 'Land_and_Ocean_LatLong1_185001_202112_ann_mean_2pt5degree.nc'\n",
    "da_obs = get_netcdf_da(nc_filename_obs)\n",
    "weights = np.cos(np.deg2rad(da_obs.lat))\n",
    "weights.name = \"weights\"\n",
    "temp_weighted = da_obs.weighted(weights)\n",
    "global_mean_obs = temp_weighted.mean((\"lon\", \"lat\"), skipna=False)\n",
    "\n",
    "data_obs = preprocess_data(da_obs, MEMBERS=None,) \n",
    "x_obs = data_obs.values.reshape((data_obs.shape[0],data_obs.shape[1]*data_obs.shape[2]))\n",
    "if exp_settings[\"anomalies\"]:\n",
    "    print('filling NaNs with zeros')\n",
    "    x_obs = np.nan_to_num(x_obs,0.)\n",
    "\n",
    "print('np.shape(x_obs) = ' + str(np.shape(x_obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485df35-2e36-455e-9bbf-b412d0acc718",
   "metadata": {},
   "source": [
    "#### Load CMIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13226df-3764-402e-a6be-9e6ea3d3d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = exp_settings[\"n_train_val_test\"][0]\n",
    "N_VAL   = exp_settings[\"n_train_val_test\"][1]\n",
    "N_TEST  = exp_settings[\"n_train_val_test\"][2]\n",
    "ALL_MEMBERS = np.arange(0,N_TRAIN+N_VAL+N_TEST)\n",
    "\n",
    "def get_cmip_data():\n",
    "    data_train, data_val, data_test = None, None, None\n",
    "    labels_train, labels_val, labels_test = None, None, None\n",
    "    years_train, years_val, years_test = None, None, None\n",
    "    target_years = []\n",
    "\n",
    "    for f in filenames:\n",
    "        print(f)\n",
    "        da = get_netcdf_da(f)\n",
    "        f_labels, f_years, f_target_year = get_labels(da, plot=exp_settings[\"show_plots\"])\n",
    "\n",
    "        # create sets of train / validaton / test\n",
    "        target_years = np.append(target_years,f_target_year)\n",
    "        data_train, labels_train, years_train = make_data_split(da, \n",
    "                                                                data_train, \n",
    "                                                                f_labels, \n",
    "                                                                f_years, \n",
    "                                                                labels_train,\n",
    "                                                                years_train,\n",
    "                                                                train_members)\n",
    "        data_val, labels_val, years_val       = make_data_split(da, \n",
    "                                                                data_val, \n",
    "                                                                f_labels, \n",
    "                                                                f_years, \n",
    "                                                                labels_val,\n",
    "                                                                years_val,\n",
    "                                                                val_members)\n",
    "        data_test, labels_test, years_test    = make_data_split(da, \n",
    "                                                                data_test, \n",
    "                                                                f_labels, \n",
    "                                                                f_years, \n",
    "                                                                labels_test,\n",
    "                                                                years_test,\n",
    "                                                                test_members)\n",
    "\n",
    "    print('---------------------------')        \n",
    "    YEARS_UNIQUE = np.unique(years_train)\n",
    "    print('data_train.shape = ' + str(np.shape(data_train)))\n",
    "    print('data_val.shape = ' + str(np.shape(data_val)))\n",
    "    print('data_test.shape = ' + str(np.shape(data_test)))\n",
    "    \n",
    "    x_train = data_train.reshape((data_train.shape[0]*data_train.shape[1],data_train.shape[2]*data_train.shape[3]))\n",
    "    x_val   = data_val.reshape((data_val.shape[0]*data_val.shape[1],data_val.shape[2]*data_val.shape[3]))\n",
    "    x_test  = data_test.reshape((data_test.shape[0]*data_test.shape[1],data_test.shape[2]*data_test.shape[3]))\n",
    "\n",
    "    y_train = labels_train.reshape((data_train.shape[0]*data_train.shape[1],))\n",
    "    y_val   = labels_val.reshape((data_val.shape[0]*data_val.shape[1],))\n",
    "    y_test  = labels_test.reshape((data_test.shape[0]*data_test.shape[1],))\n",
    "\n",
    "    y_yrs_train = years_train.reshape((data_train.shape[0]*data_train.shape[1],))\n",
    "    y_yrs_val   = years_val.reshape((data_val.shape[0]*data_val.shape[1],))\n",
    "    y_yrs_test  = years_test.reshape((data_test.shape[0]*data_test.shape[1],))\n",
    "\n",
    "    print(x_train.shape, y_train.shape, y_yrs_train.shape)\n",
    "    print(x_val.shape, y_val.shape, y_yrs_val.shape)\n",
    "    print(x_test.shape, y_test.shape, y_yrs_test.shape)  \n",
    "    \n",
    "    # make onehot vectors for training\n",
    "    if exp_settings[\"network_type\"] == 'shash2':\n",
    "        onehot_train = np.zeros((x_train.shape[0],2))\n",
    "        onehot_train[:,0] = y_train.astype('float32')\n",
    "        onehot_val = np.zeros((x_val.shape[0],2))    \n",
    "        onehot_val[:,0] = y_val.astype('float32')\n",
    "        onehot_test = np.zeros((x_test.shape[0],2))    \n",
    "        onehot_test[:,0] = y_test.astype('float32')\n",
    "    else:\n",
    "        onehot_train = np.copy(y_train)\n",
    "        onehot_val = np.copy(y_val)\n",
    "        onehot_test = np.copy(y_test)    \n",
    "    \n",
    "    map_shape = np.shape(data_train)[2:]\n",
    "    \n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test, onehot_train, onehot_val, onehot_test, y_yrs_train, y_yrs_val, y_yrs_test, target_years, map_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf95abe-3856-45d6-9204-6cb9bad3dfaf",
   "metadata": {
    "id": "6bf95abe-3856-45d6-9204-6cb9bad3dfaf",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Network and XAI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2587a9c-7d17-4744-b4b2-f7781815ef2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                   patience=exp_settings['patience'],\n",
    "                                                   verbose=1,\n",
    "                                                   mode='auto',\n",
    "                                                   restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffe81f-e03b-4fd1-bf6a-8afbfeedce72",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1646449704707,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "7dffe81f-e03b-4fd1-bf6a-8afbfeedce72",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Exponentiate(keras.layers.Layer):\n",
    "    \"\"\"Custom layer to exp the sigma and tau estimates inline.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Exponentiate, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.exp(inputs)\n",
    "\n",
    "class InterquartileCapture(tf.keras.metrics.Metric):\n",
    "    \"\"\"Compute the fraction of true values between the 25 and 75 percentiles.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, pred, sample_weight=None):\n",
    "        mu = pred[:, 0]\n",
    "        sigma = pred[:, 1]\n",
    "        norm_dist = tfp.distributions.Normal(mu,sigma)\n",
    "        lower = norm_dist.quantile(.25)\n",
    "        upper = norm_dist.quantile(.75)\n",
    "\n",
    "        batch_count = tf.reduce_sum(\n",
    "            tf.cast(\n",
    "                tf.math.logical_and(\n",
    "                    tf.math.greater(y_true[:, 0], lower),\n",
    "                    tf.math.less(y_true[:, 0], upper)\n",
    "                ),\n",
    "                tf.float32\n",
    "            )\n",
    "\n",
    "        )\n",
    "        batch_total = len(y_true[:, 0])\n",
    "\n",
    "        self.count.assign_add(tf.cast(batch_count, tf.float32))\n",
    "        self.total.assign_add(tf.cast(batch_total, tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.count / self.total\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}\n",
    "\n",
    "\n",
    "class SignTest(tf.keras.metrics.Metric):\n",
    "    \"\"\"Compute the fraction of true values above the median.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, pred, sample_weight=None):\n",
    "        mu = pred[:, 0]\n",
    "        sigma = pred[:, 1]\n",
    "        norm_dist = tfp.distributions.Normal(mu,sigma)\n",
    "        median = norm_dist.quantile(.50)\n",
    "\n",
    "        batch_count = tf.reduce_sum(\n",
    "            tf.cast(tf.math.greater(y_true[:, 0], median), tf.float32)\n",
    "        )\n",
    "        batch_total = len(y_true[:, 0])\n",
    "\n",
    "        self.count.assign_add(tf.cast(batch_count, tf.float32))\n",
    "        self.total.assign_add(tf.cast(batch_total, tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.count / self.total\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}\n",
    "\n",
    "    \n",
    "class CustomMAE(tf.keras.metrics.Metric):\n",
    "    \"\"\"Compute the prediction mean absolute error.\n",
    "\n",
    "    The \"predicted value\" is the median of the conditional distribution.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * The computation is done by maintaining running sums of total predictions\n",
    "        and correct predictions made across all batches in an epoch. The\n",
    "        running sums are reset at the end of each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.error = self.add_weight(\"error\", initializer=\"zeros\")\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, pred, sample_weight=None):\n",
    "        mu = pred[:, 0]\n",
    "        sigma = pred[:, 1]\n",
    "        norm_dist = tfp.distributions.Normal(mu,sigma)\n",
    "        predictions = norm_dist.quantile(.50)\n",
    "\n",
    "        error = tf.math.abs(y_true[:, 0] - predictions)\n",
    "        batch_error = tf.reduce_sum(error)\n",
    "        batch_total = tf.math.count_nonzero(error)\n",
    "\n",
    "        self.error.assign_add(tf.cast(batch_error, tf.float32))\n",
    "        self.total.assign_add(tf.cast(batch_total, tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.error / self.total\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd8aa1-f057-4e50-ad58-2af59f718e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressLossExpSigma(y_true, y_pred):\n",
    "    \n",
    "    # network predictions\n",
    "    mu = y_pred[:,0]\n",
    "    sigma = y_pred[:,1]\n",
    "    \n",
    "    # normal distribution defined by N(mu,sigma)\n",
    "    norm_dist = tfp.distributions.Normal(mu,sigma)\n",
    "\n",
    "    # compute the log as the -log(p)\n",
    "    loss = -norm_dist.log_prob(y_true[:,0])    \n",
    "\n",
    "    return tf.reduce_mean(loss, axis=-1)    \n",
    "\n",
    "def compile_model():\n",
    "\n",
    "    # First we start with an input layer\n",
    "    inputs = Input(shape=x_train.shape[1:]) \n",
    "\n",
    "    normalizer = tf.keras.layers.Normalization()\n",
    "    normalizer.adapt(x_train)\n",
    "    layers = normalizer(inputs)\n",
    "\n",
    "    layers = Dropout(rate=exp_settings[\"dropout_rate\"],\n",
    "                     seed=SEED)(layers) \n",
    "    \n",
    "    for hidden, activation, ridge in zip(exp_settings[\"hiddens\"], exp_settings[\"act_fun\"], exp_settings[\"ridge_param\"]):\n",
    "        layers = Dense(hidden, activation=activation,\n",
    "                       kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.00, l2=ridge),\n",
    "                       bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED),\n",
    "                       kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED))(layers)\n",
    "\n",
    "\n",
    "    if exp_settings['network_type'] == 'reg':\n",
    "        LOSS = 'mae'\n",
    "        metrics = ['mse',]\n",
    "        \n",
    "        output_layer = Dense(1, activation='linear',\n",
    "                          bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED),\n",
    "                          kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED))(layers)\n",
    "        \n",
    "    elif exp_settings['network_type'] == 'shash2':\n",
    "        LOSS = RegressLossExpSigma\n",
    "        metrics = [\n",
    "                    CustomMAE(name=\"custom_mae\"),\n",
    "                    InterquartileCapture(name=\"interquartile_capture\"),\n",
    "                    SignTest(name=\"sign_test\"),\n",
    "                  ]\n",
    "\n",
    "        y_avg = np.mean(y_train)\n",
    "        y_std = np.std(y_train)\n",
    "\n",
    "        mu_z_unit = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=\"linear\",\n",
    "            use_bias=True,\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED+100),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED+100),\n",
    "            name=\"mu_z_unit\",\n",
    "        )(layers)\n",
    "        \n",
    "        mu_unit = tf.keras.layers.Rescaling(\n",
    "            scale=y_std,\n",
    "            offset=y_avg,\n",
    "            name=\"mu_unit\",\n",
    "        )(mu_z_unit)\n",
    "        \n",
    "        # sigma_unit. The network predicts the log of the scaled sigma_z, then\n",
    "        # the resclaing layer scales it up to log of sigma y, and the custom\n",
    "        # Exponentiate layer converts it to sigma_y.\n",
    "        log_sigma_z_unit = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=\"linear\",\n",
    "            use_bias=True,\n",
    "            bias_initializer=tf.keras.initializers.Zeros(),\n",
    "            kernel_initializer=tf.keras.initializers.Zeros(),\n",
    "            name=\"log_sigma_z_unit\",\n",
    "        )(layers)\n",
    "\n",
    "        log_sigma_unit = tf.keras.layers.Rescaling(\n",
    "            scale=1.0,\n",
    "            offset=np.log(y_std),\n",
    "            name=\"log_sigma_unit\",\n",
    "        )(log_sigma_z_unit)\n",
    "\n",
    "        sigma_unit = Exponentiate(\n",
    "            name=\"sigma_unit\",\n",
    "        )(log_sigma_unit)\n",
    "        \n",
    "        output_layer = tf.keras.layers.concatenate([mu_unit, sigma_unit], axis=1)\n",
    "        \n",
    "    else:\n",
    "        raise NotImpletementedError('no such network_type')\n",
    "        \n",
    "    # Constructing the model\n",
    "    model = Model(inputs, output_layer)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=exp_settings[\"learning_rate\"]), \n",
    "                  loss=LOSS, \n",
    "                  metrics=metrics,\n",
    "                 )\n",
    "        \n",
    "        \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da52f05-ac5c-4ab5-9632-0bd82507dcc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gradients(inputs, top_pred_idx=None):\n",
    "    \"\"\"Computes the gradients of outputs w.r.t input image.\n",
    "\n",
    "    Args:\n",
    "        inputs: 2D/3D/4D matrix of samples\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.\n",
    "\n",
    "    Returns:\n",
    "        Gradients of the predictions w.r.t img_input\n",
    "    \"\"\"\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)\n",
    "        \n",
    "        # Run the forward pass of the layer and record operations\n",
    "        # on GradientTape.\n",
    "        preds = model(inputs, training=False)  \n",
    "        \n",
    "        # For classification, grab the top class\n",
    "        if top_pred_idx is not None:\n",
    "            preds = preds[:, top_pred_idx]\n",
    "        \n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.        \n",
    "    grads = tape.gradient(preds, inputs)\n",
    "    return grads\n",
    "\n",
    "def get_integrated_gradients(inputs, baseline=None, num_steps=50, top_pred_idx=None):\n",
    "    \"\"\"Computes Integrated Gradients for a prediction.\n",
    "\n",
    "    Args:\n",
    "        inputs (ndarray): 2D/3D/4D matrix of samples\n",
    "        baseline (ndarray): The baseline image to start with for interpolation\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.            \n",
    "\n",
    "    Returns:\n",
    "        Integrated gradients w.r.t input image\n",
    "    \"\"\"\n",
    "    # If baseline is not provided, start with zeros\n",
    "    # having same size as the input image.\n",
    "    if baseline is None:\n",
    "        input_size = np.shape(inputs)[1:]\n",
    "        baseline = np.zeros(input_size).astype(np.float32)\n",
    "    else:\n",
    "        baseline = baseline.astype(np.float32)\n",
    "\n",
    "    # 1. Do interpolation.\n",
    "    inputs = inputs.astype(np.float32)\n",
    "    interpolated_inputs = [\n",
    "        baseline + (step / num_steps) * (inputs - baseline)\n",
    "        for step in range(num_steps + 1)\n",
    "    ]\n",
    "    interpolated_inputs = np.array(interpolated_inputs).astype(np.float32)\n",
    "\n",
    "    # 3. Get the gradients\n",
    "    grads = []\n",
    "    for i, x_data in enumerate(interpolated_inputs):\n",
    "        grad = get_gradients(x_data, top_pred_idx=top_pred_idx)\n",
    "        grads.append(grad[0])\n",
    "    grads = tf.convert_to_tensor(grads, dtype=tf.float32)\n",
    "\n",
    "    # 4. Approximate the integral using the trapezoidal rule\n",
    "    grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "    avg_grads = tf.reduce_mean(grads, axis=0)\n",
    "\n",
    "    # 5. Calculate integrated gradients and return\n",
    "    integrated_grads = (inputs - baseline) * avg_grads\n",
    "    return integrated_grads\n",
    "\n",
    "def random_baseline_integrated_gradients(inputs, num_steps=50, num_runs=5, top_pred_idx=None):\n",
    "    \"\"\"Generates a number of random baseline images.\n",
    "\n",
    "    Args:\n",
    "        inputs (ndarray): 2D/3D/4D matrix of samples\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "        num_runs: number of baseline images to generate\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.      \n",
    "\n",
    "    Returns:\n",
    "        Averaged integrated gradients for `num_runs` baseline images\n",
    "    \"\"\"\n",
    "    # 1. List to keep track of Integrated Gradients (IG) for all the images\n",
    "    integrated_grads = []\n",
    "\n",
    "    # 2. Get the integrated gradients for all the baselines\n",
    "    for run in range(num_runs):\n",
    "        baseline = np.zeros(np.shape(inputs)[1:])\n",
    "        for i in np.arange(0,np.shape(random_baseline)[0]):\n",
    "            j = np.random.choice(np.arange(0,np.shape(inputs)[0]))\n",
    "            baseline[i] = inputs[j,i]\n",
    "\n",
    "        igrads = get_integrated_gradients(\n",
    "            inputs=inputs,\n",
    "            baseline=baseline,\n",
    "            num_steps=num_steps,\n",
    "        )\n",
    "        integrated_grads.append(igrads)\n",
    "\n",
    "    # 3. Return the average integrated gradients for the image\n",
    "    integrated_grads = tf.convert_to_tensor(integrated_grads)\n",
    "    return tf.reduce_mean(integrated_grads, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807abd7-832a-484b-98cd-7e6c3a9f60c0",
   "metadata": {
    "id": "c807abd7-832a-484b-98cd-7e6c3a9f60c0",
    "tags": []
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becb266-c9fd-4098-a2ba-e6c52804b8bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "executionInfo": {
     "elapsed": 105064,
     "status": "ok",
     "timestamp": 1646449809976,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "7becb266-c9fd-4098-a2ba-e6c52804b8bd",
    "outputId": "5f2d4b54-fb88-418f-95a2-3c5e281cc2e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(exp_settings[\"rng_seed\"])\n",
    "pred_obs_vec = np.zeros(shape=(exp_settings['n_models'], x_obs.shape[0], 2))*np.nan\n",
    "\n",
    "for iloop in np.arange(exp_settings['n_models']):\n",
    "    SEED = rng.integers(low=1_000,high=10_000,size=1)[0]    \n",
    "    tf.random.set_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    train_members = rng.choice(ALL_MEMBERS, size=N_TRAIN, replace=False)\n",
    "    val_members   = rng.choice(np.setdiff1d(ALL_MEMBERS,train_members), size=N_VAL, replace=False)\n",
    "    test_members  = rng.choice(np.setdiff1d(ALL_MEMBERS,np.append(train_members[:],val_members)), size=N_TEST, replace=False)\n",
    "    print(train_members, val_members, test_members)\n",
    "\n",
    "    (x_train, \n",
    "     x_val, \n",
    "     x_test, \n",
    "     y_train, \n",
    "     y_val, \n",
    "     y_test, \n",
    "     onehot_train, \n",
    "     onehot_val, \n",
    "     onehot_test, \n",
    "     y_yrs_train, \n",
    "     y_yrs_val, \n",
    "     y_yrs_test, \n",
    "     target_years, \n",
    "     map_shape) = get_cmip_data()\n",
    "\n",
    "    #----------------------------------------        \n",
    "    tf.keras.backend.clear_session()                \n",
    "    model = compile_model()\n",
    "    history = model.fit(x_train, onehot_train, \n",
    "                        epochs=exp_settings['n_epochs'], \n",
    "                        verbose=exp_settings['verbosity'],\n",
    "                        batch_size = exp_settings['batch_size'], \n",
    "                        shuffle=True,\n",
    "                        validation_data=[x_val, onehot_val],\n",
    "                        callbacks=[early_stopping,],\n",
    "                       )\n",
    "    #----------------------------------------\n",
    "    # create predictions for observations with this model\n",
    "    pred_obs = model.predict(x_obs)\n",
    "    pred_obs_vec[iloop,:,:pred_obs.shape[1]] = pred_obs\n",
    "    \n",
    "    #----------------------------------------\n",
    "    # save the tensorflow model\n",
    "    model_name = (exp_settings[\"exp_name\"] + '_' +\n",
    "                  'ssp' + exp_settings[\"ssp\"] + '_' +\n",
    "                  exp_settings[\"network_type\"] + \n",
    "                  '_rng' + str(exp_settings[\"rng_seed\"]) + \n",
    "                  '_seed' + str(SEED)\n",
    "                 )\n",
    "    if exp_settings[\"save_model\"]:\n",
    "        save_tf_model(model, model_name)\n",
    "        save_pred_obs(pred_obs_vec, model_name[:model_name.rfind('_seed')] + '_pred_obs')\n",
    "    \n",
    "    #----------------------------------------\n",
    "    if exp_settings[\"show_plots\"]:\n",
    "       \n",
    "        plt.plot(history.history['loss'], label='loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635d170-ee55-4c82-8519-93b2565fcd5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make loss plots following training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0f89f-a5b7-4e32-bae4-4f0f9efa73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history,metric):\n",
    "    \n",
    "    imin = np.argmin(history.history['val_loss'])\n",
    "    \n",
    "    plt.plot(history.history[metric], label='training')\n",
    "    plt.plot(history.history['val_' + metric], label='validation')\n",
    "    plt.title(metric)\n",
    "    plt.axvline(x=imin, linewidth=.5, color='gray',alpha=.5)\n",
    "    plt.legend()\n",
    "\n",
    "if exp_settings[\"network_type\"] == 'shash2':\n",
    "    try:\n",
    "        imin = len(history.history['custom_mae'])\n",
    "        plt.subplots(figsize=(20,4))\n",
    "\n",
    "        plt.subplot(1,4,1)\n",
    "        plot_metrics(history,'loss')\n",
    "        plt.ylim(0,10.)\n",
    "\n",
    "        plt.subplot(1,4,2)\n",
    "        plot_metrics(history,'custom_mae')\n",
    "        plt.ylim(0,10)\n",
    "\n",
    "        plt.subplot(1,4,3)\n",
    "        plot_metrics(history,'interquartile_capture')\n",
    "\n",
    "        plt.subplot(1,4,4)\n",
    "        plot_metrics(history,'sign_test')\n",
    "\n",
    "        plt.show()\n",
    "    except:\n",
    "        print('metrics were not saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647acbe8-c4f3-428a-95ca-e40373ec1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_settings['network_type'] == \"shash2\":\n",
    "    top_pred_idx = 0\n",
    "else:\n",
    "    top_pred_idx = None\n",
    "    \n",
    "YEARS_UNIQUE = np.unique(y_yrs_train)\n",
    "predict_train = model.predict(x_train)[:,top_pred_idx].flatten()\n",
    "predict_val = model.predict(x_val)[:,top_pred_idx].flatten()\n",
    "mae = np.mean(np.abs(predict_val-y_val[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b05d0-da60-4eda-86b7-4327333093c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1646449810953,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "156b05d0-da60-4eda-86b7-4327333093c1",
    "outputId": "6d37c46a-c8a3-4e44-df23-edd7e9b38697",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clr = ('tab:purple','tab:orange', 'tab:pink', 'tab:green', 'gold', 'violet','cornflowerblue','darkorange')\n",
    "#--------------------------------\n",
    "plt.subplots(1,2,figsize=(15,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(y_train, predict_train,'.',color='gray',alpha=.5, label='training')\n",
    "plt.plot(y_val, predict_val,'.', label='validation')\n",
    "plt.plot(y_val,y_val,'--',color='fuchsia')\n",
    "plt.axvline(x=0,color='gray',linewidth=1)\n",
    "plt.axhline(y=0,color='gray',linewidth=1)\n",
    "plt.title('Validation MAE = ' + str(mae.round(2)) + ' years')\n",
    "plt.xlabel('true number of years until target is reached')\n",
    "plt.ylabel('predicted number of years until target is reached')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(y_yrs_train, predict_train,'.',color='gray',alpha=.5, label='training')\n",
    "plt.xlabel('year of map')\n",
    "plt.ylabel('predicted number of years until target is reached')\n",
    "plt.axhline(y=0, color='gray', linewidth=1)\n",
    "\n",
    "predict_val_mat = predict_val.reshape(N_GCMS,N_VAL,len(YEARS_UNIQUE))\n",
    "for i in np.arange(0,predict_val_mat.shape[0]):\n",
    "    plt.plot(YEARS_UNIQUE, predict_val_mat[i,:,:].swapaxes(1,0),'.', label='validation', color=clr[i])\n",
    "    plt.axvline(x=target_years[i],linestyle='--',color=clr[i])\n",
    "if IN_COLAB==False:\n",
    "    pass\n",
    "    # plt.savefig('figures/initial_result_seed' + str(SEED) + '.png', dpi=savefig_dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ad419-afcc-4ab8-8a4a-d5cb3ca15c78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Predict observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe28ad8-862a-401c-97c1-f8eb44f05038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_predict_obs = model.predict(x_obs_predict)[:,top_pred_idx].flatten()\n",
    "\n",
    "iy = np.where(da_obs['time.year'].values >= 2001)[0]\n",
    "x = da_obs['time.year'].values[iy]\n",
    "y = y_predict_obs[iy]\n",
    "linear_model = stats.linregress(x=x,y=y)\n",
    "\n",
    "#--------------------------------\n",
    "i_year = np.where(y_predict_obs < 0)[0]\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(da_obs['time.year'], y_predict_obs, '.r')\n",
    "plt.plot(x, linear_model.slope*x+linear_model.intercept, '--k', alpha=.5, linewidth=2)\n",
    "plt.xlabel('year of map')\n",
    "plt.ylabel('predicted number of years \\nuntil target is reached')\n",
    "plt.title('Observations Target Year = ' + \n",
    "          str(np.round(2021+y_predict_obs[-1],1)) +\n",
    "          ' (' + str(y_predict_obs[-1].round()) + ' years)'+\n",
    "          '\\n slope = ' + str(linear_model.slope.round(2)) \n",
    "         )\n",
    "plt.xlim(1970,2025)\n",
    "# plt.ylim(-20,40)\n",
    "plt.axhline(y=0,color='gray')\n",
    "\n",
    "#--------------------------------\n",
    "plt.subplot(1,2,2)\n",
    "global_mean_obs.plot(linewidth=2,label='data',color=\"tab:orange\")\n",
    "plt.title('Observations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e3dd0-7bd4-4271-8af1-850cb1aee64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plot = 4\n",
    "grads_obs = get_gradients(x_obs[-n_plot:,:],top_pred_idx=top_pred_idx).numpy()*x_obs[-n_plot:,:]\n",
    "print('np.shape(grads_obs) = ' + str(np.shape(grads_obs)))\n",
    "grads_obs_mean = np.mean(grads_obs,axis=0)\n",
    "plt.figure(figsize=(10,4))\n",
    "plot_map(grads_obs_mean.reshape((map_shape[0],map_shape[1])), \n",
    "         clim = (-.02,.02),\n",
    "         title = 'Observations ' + str(2021-n_plot+1) + '-' + str(2021) + ': Gradient x Input',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f952d05-2b3d-4705-91a9-1deb752591fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "\n",
    "if exp_settings[\"network_type\"] == 'shash2':\n",
    "\n",
    "    clr_choice = 'orange'\n",
    "    y_predict_obs = model.predict(x_obs_predict)\n",
    "\n",
    "    iy = np.where(da_obs['time.year'].values >= 2011)[0]\n",
    "    x = da_obs['time.year'].values[iy]\n",
    "    y = y_predict_obs[iy,0]\n",
    "    linear_model = stats.linregress(x=x,y=y)\n",
    "\n",
    "    #--------------------------------\n",
    "    norm_incs = np.arange(-80,80,1)\n",
    "    mu_pred = y_predict_obs[:,0]\n",
    "    sigma_pred = y_predict_obs[:,1]\n",
    "    norm_dist = tfp.distributions.Normal(mu_pred,sigma_pred)\n",
    "    norm_perc_low = norm_dist.quantile(.25).numpy()   \n",
    "    norm_perc_high = norm_dist.quantile(.75).numpy()      \n",
    "    norm_perc_med = norm_dist.quantile(.5).numpy()      \n",
    "    norm_cpd = norm_dist[-1].prob(norm_incs)\n",
    "    y_predict_obs = norm_perc_med\n",
    "    \n",
    "    print('2021 prediction = ' + str(mu_pred[-1]) + ' (' + str(norm_perc_low[-1]) + ' to ' + str(norm_perc_high[-1]) + ')')\n",
    "\n",
    "    #------------------------------------------------------------\n",
    "    ax = plt.subplots(1,2,figsize=(16,4))\n",
    "    years = np.arange(1850,2022)\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    for iyear in np.arange(0,y_predict_obs.shape[0]):\n",
    "        min_val = norm_perc_low[iyear]\n",
    "        max_val = norm_perc_high[iyear]\n",
    "\n",
    "        if(years[iyear]==2021):\n",
    "            clr = clr_choice\n",
    "        else:\n",
    "            clr = 'gray'\n",
    "        plt.plot((years[iyear],years[iyear]),(min_val, max_val),\n",
    "                 linestyle='-',\n",
    "                 linewidth=4,\n",
    "                 color=clr,\n",
    "                )\n",
    "\n",
    "    plt.plot(x,x*linear_model.slope+linear_model.intercept,'--', color='black')\n",
    "\n",
    "    plt.xlim(1970.5,2023)    \n",
    "    plt.ylim(0,57)\n",
    "    plt.ylabel('years until target')\n",
    "    plt.xlabel('year')\n",
    "    plt.title('Observations predictions under SSP337 (norm)\\n slope=' + str(linear_model.slope.round(2)))\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(norm_incs,norm_cpd,\n",
    "             linewidth=5,\n",
    "             color=clr_choice,\n",
    "            )\n",
    "\n",
    "    k = np.argmin(np.abs(norm_perc_low[-1]-norm_incs))\n",
    "    plt.plot((norm_perc_low[-1],norm_perc_low[-1]),(0,norm_cpd[k]),'--',color=clr_choice)\n",
    "    k = np.argmin(np.abs(norm_perc_high[-1]-norm_incs))\n",
    "    plt.plot((norm_perc_high[-1],norm_perc_high[-1]),(0,norm_cpd[k]),'--',color=clr_choice)\n",
    "\n",
    "    plt.xlabel('years until target')\n",
    "    plt.title('Predictions for Observations\\nYear = 2021')\n",
    "    plt.xlim(-10,40)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ea61d-e514-415e-be5a-15b43367134f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Explainability via Input * Gradient and Integrated Gradients\n",
    "We will use two attribution explainaiblity methods called Input * Gradient and Integrated Gradients to make heatmaps of regions of the input that act as explanations for the network's prediction.\n",
    "\n",
    "* https://keras.io/examples/vision/integrated_gradients/\n",
    "* https://distill.pub/2020/attribution-baselines/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364bda5b-6cc2-4117-8820-44cdf07b4c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_map(x, clim=None, title=None, text=None, cmap='RdGy'):\n",
    "    plt.pcolor(x,\n",
    "               cmap=cmap,\n",
    "              )\n",
    "    plt.clim(clim)\n",
    "    plt.colorbar()\n",
    "    plt.title(title,fontsize=15,loc='right')    \n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.text(0.01, 1.0, text, fontfamily='monospace', fontsize='small', va='bottom',transform=plt.gca().transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef431b-27ad-4874-959e-169e6f7f198c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#=========================================\n",
    "# Define the samples you want to explain\n",
    "rng = np.random.default_rng(45)\n",
    "isubsample = rng.choice(np.arange(0,x_val.shape[0]),\n",
    "                        size = 500,\n",
    "                        replace = False,\n",
    "                       )\n",
    "\n",
    "inputs = np.copy(x_val[isubsample,:])\n",
    "targets = np.copy(y_val[isubsample])\n",
    "yrs = np.copy(y_yrs_val[isubsample])\n",
    "preds = model.predict(inputs)\n",
    "\n",
    "#=========================================\n",
    "#---------------------------------------\n",
    "# Gradient x Input\n",
    "#---------------------------------------\n",
    "# compute the multiplication of gradient * inputs\n",
    "# and reshape into a map of latitude x longitude\n",
    "\n",
    "grads = get_gradients(inputs,top_pred_idx).numpy()\n",
    "grad_x_input = grads * inputs\n",
    "grad_x_input = grad_x_input.reshape((len(targets),map_shape[0],map_shape[1]))\n",
    "print(np.shape(grad_x_input))\n",
    "\n",
    "#---------------------------------------\n",
    "# Integrated Gradients\n",
    "#---------------------------------------\n",
    "baseline_mean = np.mean(x_train,axis=0)*0.    \n",
    "print('shape(baseline_mean) = ' + str(np.shape(baseline_mean)))\n",
    "print('model.predict(baseline_mean) = ' + str(model.predict(baseline_mean[np.newaxis,:])))\n",
    "\n",
    "igrad = get_integrated_gradients(inputs, baseline=baseline_mean,top_pred_idx=top_pred_idx)\n",
    "integrated_gradients = igrad.numpy().reshape((len(targets),map_shape[0],map_shape[1]))\n",
    "print(np.shape(integrated_gradients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c536cb-85e5-4b49-84ab-7de73aa82a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot the resulting heatmaps for a subset of samples\n",
    "# based on their label\n",
    "plot_list = (40, 20, 10, 0)\n",
    "NCOL = 4\n",
    "plt.subplots(len(plot_list),NCOL,figsize=(35,5*len(plot_list)))\n",
    "\n",
    "for irow,min_range in enumerate(plot_list):\n",
    "        \n",
    "    max_range = min_range + 5\n",
    "    isamples = np.where((targets >= min_range) & (targets <= max_range))[0]\n",
    "    igrad_mean = np.mean(integrated_gradients[isamples,:,:],axis=0)\n",
    "    grad_x_input_mean = np.mean(grad_x_input[isamples,:,:],axis=0)\n",
    "    grad_mean = np.mean(grads[isamples,:],axis=0).reshape((map_shape[0],map_shape[1]))\n",
    "    x_inputs_mean = np.mean(inputs[isamples,:],axis=0).reshape((map_shape[0],map_shape[1]))\n",
    "    x_inputs_mean = x_inputs_mean - baseline_mean.reshape((map_shape[0],map_shape[1]))\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    text = (\n",
    "            \"\\n\"\n",
    "            + f\"  label_range    = {min_range}-{max_range} yrs.\\n\"                    \n",
    "            + f\"  n_samples      = {len(isamples)}\\n\"\n",
    "    )    \n",
    "    #------------------------------------------------------------------    \n",
    "    \n",
    "    # plot average input map\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+1)\n",
    "    plot_map(x_inputs_mean, \n",
    "             text=text,\n",
    "             clim=(-5,5),\n",
    "             cmap='RdBu_r',\n",
    "             title = 'Temperature anomaly from Baseline',\n",
    "            )\n",
    "    #------------------------------------------------------------------\n",
    "    # plot explainability of gradient (saliency)\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+2)\n",
    "    plot_map(grad_mean, \n",
    "             text=text,             \n",
    "             clim=(-0.02, .02), \n",
    "             title = 'Gradient (Saliency)',\n",
    "            )\n",
    "    \n",
    "    #------------------------------------------------------------------\n",
    "    # plot explainability of input x gradient\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+3)\n",
    "    plot_map(grad_x_input_mean, \n",
    "             text=text,\n",
    "             clim=(-.02,.02),\n",
    "             title = 'Gradient x Input',\n",
    "            )\n",
    "\n",
    "    #------------------------------------------------------------------\n",
    "    # plot explainability of integrated gradients\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+4)\n",
    "    plot_map(igrad_mean, \n",
    "             text=text,             \n",
    "             clim=(-.02,.02), \n",
    "             title = 'Integrated Gradients',\n",
    "            )\n",
    "\n",
    "plt.tight_layout()   \n",
    "if IN_COLAB==False:\n",
    "    pass\n",
    "    # plt.savefig('figures/xai_grid_' + str(min_range) +'-' + str(max_range) + '_baseline_' + str(BASELINE) + '.png', dpi=savefig_dpi)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e94ba-f432-496a-a963-98a3afc3d1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "_main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
