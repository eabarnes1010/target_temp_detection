{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a650402-4774-49cb-9b72-9c8f1dd02f1d",
   "metadata": {
    "id": "4a650402-4774-49cb-9b72-9c8f1dd02f1d"
   },
   "source": [
    "# Detecting temperature targets\n",
    "[![Latest release](https://badgen.net/github/release/Naereen/Strapdown.js)](https://github.com/eabarnes1010/target_temp_detection/tree/main)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eabarnes1010/target_temp_detection/blob/main/_train_model.ipynb)\n",
    "\n",
    "_(ignore the badges for now - they are for when I post on github)_\n",
    "##### authors: Elizabeth A. Barnes and Noah Diffenbaugh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fc82c-3e30-4466-b896-1b292d5b2ef0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Python setup stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811e52c-15c4-4e8a-8cb9-da43cb4ec4ee",
   "metadata": {
    "id": "c811e52c-15c4-4e8a-8cb9-da43cb4ec4ee",
    "tags": []
   },
   "source": [
    "The following code is for setting up a local environment.\n",
    "```\n",
    "conda create --name env-noah python=3.9\n",
    "conda activate env-noah\n",
    "pip install tensorflow==2.7.0\n",
    "pip install tensorflow-probability==0.15.0\n",
    "pip install --upgrade numpy scipy pandas statsmodels matplotlib seaborn palettable progressbar2 tabulate icecream flake8 keras-tuner sklearn jupyterlab black isort jupyterlab_code_formatter\n",
    "pip install -U scikit-learn\n",
    "pip install silence-tensorflow tqdm\n",
    "conda install -c conda-forge cmocean cartopy\n",
    "conda install -c conda-forge xarray dask netCDF4 bottleneck\n",
    "conda install -c conda-forge nc-time-axis\n",
    "```\n",
    "\n",
    "Use the command\n",
    "```python -m pip freeze > requirements.txt```\n",
    "to make a pip installation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc0268-dd0a-4296-8a0b-f14b4c6189b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1646449663380,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "1fdc0268-dd0a-4296-8a0b-f14b4c6189b2",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "4e614eaa-b436-4f26-f1bd-3b59fcad893e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "print('IN_COLAB = ' + str(IN_COLAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb968382-4186-466e-a85b-b00caa5fc9be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17642,
     "status": "ok",
     "timestamp": 1646449680995,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "fb968382-4186-466e-a85b-b00caa5fc9be",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "d7964af9-2d52-4466-902d-9b85faba9a91",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install xarray==0.20.2\n",
    "    !pip install nc-time-axis\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Softmax\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "savefig_dpi = 300\n",
    "np.warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5cee3-6f4f-4818-92e1-1351eeeb565a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1646449681009,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "29a5cee3-6f4f-4818-92e1-1351eeeb565a",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "e5f5b0ac-82b8-4147-bf44-4bc3b49466a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"python version = {sys.version}\")\n",
    "print(f\"numpy version = {np.__version__}\")\n",
    "print(f\"xarray version = {xr.__version__}\")  \n",
    "print(f\"tensorflow version = {tf.__version__}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de167a6-1c3f-435f-815c-0ff078ef9d1e",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3faaa-4f55-4d7f-a538-6c641098687f",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1646449681009,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "a0d3faaa-4f55-4d7f-a538-6c641098687f"
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = 'data/'\n",
    "filenames = ('tas_Amon_historical_ssp370_CanESM5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "             'tas_Amon_historical_ssp370_GISS-E2-1-G_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "             'tas_Amon_historical_ssp370_IPSL-CM6A-LR_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "             'tas_Amon_historical_ssp370_UKESM1-0-LL_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "             'tas_Amon_historical_ssp370_ACCESS-ESM1-5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n",
    "            )\n",
    "N_MODELS = len(filenames)\n",
    "N_TRAIN, N_VAL, N_TEST = (7,3,0)\n",
    "\n",
    "TARGET_TEMP = 1.5\n",
    "BASELINE_YEARS = ('1850','1899')\n",
    "START_YEAR = 1960\n",
    "LAT_START = -90\n",
    "REMOVE_MEAN = None # OPTIONS: None, 'weighted', 'raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53d22e-95cb-4a2e-86b7-05dd68e5136e",
   "metadata": {
    "id": "ca53d22e-95cb-4a2e-86b7-05dd68e5136e",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Define the target year and labels\n",
    "\n",
    "For starters, we will define the target year as the year that the ensemble mean, global mean temperature crosses the ```TARGET_TEMP``` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae33333-64de-4bad-86c8-73b31083dacb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "executionInfo": {
     "elapsed": 2814,
     "status": "ok",
     "timestamp": 1646449704589,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "aae33333-64de-4bad-86c8-73b31083dacb",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "195a23ad-e907-478f-d6bf-758b287e3068",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_labels(da, plot=False):\n",
    "    # compute the ensemble mean, global mean temperature\n",
    "    # these computations should be based on the training set only\n",
    "    da_ens = da.mean(axis=0)\n",
    "    weights = np.cos(np.deg2rad(da_ens.lat))\n",
    "    weights.name = \"weights\"\n",
    "    temp_weighted = da_ens.weighted(weights)\n",
    "    global_mean = temp_weighted.mean((\"lon\", \"lat\"))\n",
    "    \n",
    "    global_mean_ens = da.weighted(weights)\n",
    "    global_mean_ens = global_mean_ens.mean((\"lon\",\"lat\"))\n",
    "    \n",
    "    # compute the target year \n",
    "    baseline_mean = global_mean.sel(time=slice(BASELINE_YEARS[0],BASELINE_YEARS[1])).mean('time')\n",
    "    iwarmer = np.where(global_mean.values > baseline_mean.values+TARGET_TEMP)[0]\n",
    "    target_year = global_mean[\"time\"].values[iwarmer[0]]\n",
    "\n",
    "\n",
    "    # plot the calculation to make sure things make sense\n",
    "    if plot == True:\n",
    "        for ens in np.arange(0,global_mean_ens.shape[0]):\n",
    "            global_mean_ens[ens,:].plot(linewidth=1.0,color=\"gray\")\n",
    "        global_mean.plot(linewidth=2,label='data',color=\"aqua\")\n",
    "        plt.axhline(y=baseline_mean, color='k', linestyle='-', label='baseline temp')\n",
    "        plt.axhline(y=baseline_mean+TARGET_TEMP, color='tab:blue',linewidth=1., linestyle='--', label='target temp')\n",
    "        plt.axvline(x=target_year,color='tab:blue',linewidth=1., linestyle='--', label='target year')\n",
    "        global_mean_obs.plot(linewidth=2,label='data',color=\"tab:orange\")        \n",
    "        plt.xlabel('year')\n",
    "        plt.ylabel('temp (K)')\n",
    "        plt.title(f + '\\ntargets [' + str(target_year.year) + ', ' + str(TARGET_TEMP) + 'C]',\n",
    "                  fontsize = 8,\n",
    "                 )\n",
    "        # plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # define the labels\n",
    "    print('TARGET_YEAR = ' + str(target_year.year))\n",
    "    labels = target_year.year - da['time.year'].values\n",
    "    # print('labels = ' + str(labels))\n",
    "    \n",
    "    return labels, da['time.year'].values, target_year.year\n",
    "\n",
    "def preprocess_data(da, MEMBERS):\n",
    "\n",
    "    i_lat_start = np.where(da['lat']>=LAT_START)[0][0]\n",
    "    if MEMBERS is None:\n",
    "        new_data = da[:,i_lat_start:,:]\n",
    "    else:\n",
    "        new_data = da[MEMBERS,:,i_lat_start:,:]\n",
    "    \n",
    "    if REMOVE_MEAN == 'raw':\n",
    "        new_data = new_data - new_data.mean((\"lon\",\"lat\"))\n",
    "    elif REMOVE_MEAN == 'weighted':\n",
    "        weights = np.cos(np.deg2rad(new_data.lat))\n",
    "        weights.name = \"weights\"\n",
    "        new_data_weighted = new_data.weighted(weights)\n",
    "        new_data = new_data - new_data_weighted.mean((\"lon\",\"lat\"))\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "def make_data_split(da, data, f_labels, f_years, labels, years, MEMBERS):\n",
    "\n",
    "    # only train on certain samples\n",
    "    iyears = np.where(f_years >= START_YEAR)[0]\n",
    "    f_years = f_years[iyears]\n",
    "    f_labels = f_labels[iyears]        \n",
    "    \n",
    "    # process the data, i.e. subtract the mean, etc.\n",
    "    new_data = preprocess_data(da, MEMBERS)    \n",
    "    new_data = new_data[:,iyears,:,:]\n",
    "    \n",
    "    if data is None:\n",
    "        data = new_data.values\n",
    "        labels = np.tile(f_labels,(len(MEMBERS),1))        \n",
    "        years = np.tile(f_years,(len(MEMBERS),1))\n",
    "    else:\n",
    "        data = np.concatenate((data,new_data.values),axis=0)        \n",
    "        labels = np.concatenate((labels,np.tile(f_labels,(len(MEMBERS),1))),axis=0)        \n",
    "        years = np.concatenate((years,np.tile(f_years,(len(MEMBERS),1))),axis=0)\n",
    "    \n",
    "    return data, labels, years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9fda9f-f699-4346-acdf-0ccc38c166cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3135663-b77f-41d6-b8d6-bf54a9c87c67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nc_filename_obs = DATA_DIRECTORY + 'Land_and_Ocean_LatLong1_185001_202112_ann_mean_2pt5degree.nc'\n",
    "da_obs = xr.open_dataset(nc_filename_obs)['tas']\n",
    "# da_obs = da_obs.fillna(281.81423285)\n",
    "weights = np.cos(np.deg2rad(da_obs.lat))\n",
    "weights.name = \"weights\"\n",
    "temp_weighted = da_obs.weighted(weights)\n",
    "global_mean_obs = temp_weighted.mean((\"lon\", \"lat\"), skipna=False)\n",
    "\n",
    "data_obs = preprocess_data(da_obs, MEMBERS=None,) \n",
    "x_obs = data_obs.values.reshape((data_obs.shape[0],data_obs.shape[1]*data_obs.shape[2]))\n",
    "print('np.shape(x_obs) = ' + str(np.shape(x_obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780d7da-8875-47aa-a4fb-b7c25b251e66",
   "metadata": {},
   "source": [
    "### Get CMIP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b3327d-d557-424d-9030-c7ad3ffedb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val, data_test = None, None, None\n",
    "labels_train, labels_val, labels_test = None, None, None\n",
    "years_train, years_val, years_test = None, None, None\n",
    "target_years = []\n",
    "\n",
    "TRAIN_MEMBERS = np.arange(0,N_TRAIN)\n",
    "VAL_MEMBERS   = np.arange(N_TRAIN,N_TRAIN+N_VAL)\n",
    "TEST_MEMBERS  = np.arange(N_TRAIN+N_VAL,N_TRAIN+N_VAL+N_TEST)\n",
    "print(TRAIN_MEMBERS, VAL_MEMBERS, TEST_MEMBERS)\n",
    "\n",
    "for f in filenames:\n",
    "    print(f)\n",
    "    da = xr.open_dataarray(DATA_DIRECTORY + f)\n",
    "    f_labels, f_years, f_target_year = get_labels(da, plot=True)\n",
    "        \n",
    "    # create sets of train / validaton / test\n",
    "    target_years = np.append(target_years,f_target_year)\n",
    "    data_train, labels_train, years_train = make_data_split(da, \n",
    "                                                            data_train, \n",
    "                                                            f_labels, \n",
    "                                                            f_years, \n",
    "                                                            labels_train,\n",
    "                                                            years_train,\n",
    "                                                            TRAIN_MEMBERS)\n",
    "    data_val, labels_val, years_val       = make_data_split(da, \n",
    "                                                            data_val, \n",
    "                                                            f_labels, \n",
    "                                                            f_years, \n",
    "                                                            labels_val,\n",
    "                                                            years_val,\n",
    "                                                            VAL_MEMBERS)\n",
    "    data_test, labels_test, years_test    = make_data_split(da, \n",
    "                                                            data_test, \n",
    "                                                            f_labels, \n",
    "                                                            f_years, \n",
    "                                                            labels_test,\n",
    "                                                            years_test,\n",
    "                                                            TEST_MEMBERS)\n",
    "    \n",
    "print('---------------------------')        \n",
    "YEARS_UNIQUE = np.unique(years_train)\n",
    "print('data_train.shape = ' + str(np.shape(data_train)))\n",
    "print('data_val.shape = ' + str(np.shape(data_val)))\n",
    "print('data_test.shape = ' + str(np.shape(data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2e284-af2a-4e2d-bacc-2348839bd0a7",
   "metadata": {},
   "source": [
    "#### Reshape the data to get it ready for input into the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf322314-2f80-45c7-b60a-15c5b76a06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train.reshape((data_train.shape[0]*data_train.shape[1],data_train.shape[2]*data_train.shape[3]))\n",
    "x_val   = data_val.reshape((data_val.shape[0]*data_val.shape[1],data_val.shape[2]*data_val.shape[3]))\n",
    "x_test  = data_test.reshape((data_test.shape[0]*data_test.shape[1],data_test.shape[2]*data_test.shape[3]))\n",
    "\n",
    "y_train = labels_train.reshape((data_train.shape[0]*data_train.shape[1],))\n",
    "y_val   = labels_val.reshape((data_val.shape[0]*data_val.shape[1],))\n",
    "y_test  = labels_test.reshape((data_test.shape[0]*data_test.shape[1],))\n",
    "\n",
    "y_yrs_train = years_train.reshape((data_train.shape[0]*data_train.shape[1],))\n",
    "y_yrs_val   = years_val.reshape((data_val.shape[0]*data_val.shape[1],))\n",
    "y_yrs_test  = years_test.reshape((data_test.shape[0]*data_test.shape[1],))\n",
    "\n",
    "print(x_train.shape, y_train.shape, y_yrs_train.shape)\n",
    "print(x_val.shape, y_val.shape, y_yrs_val.shape)\n",
    "print(x_test.shape, y_test.shape, y_yrs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf95abe-3856-45d6-9204-6cb9bad3dfaf",
   "metadata": {
    "id": "6bf95abe-3856-45d6-9204-6cb9bad3dfaf",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup the network and associated callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffe81f-e03b-4fd1-bf6a-8afbfeedce72",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1646449704707,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "7dffe81f-e03b-4fd1-bf6a-8afbfeedce72",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the model and its compile function\n",
    "def compile_model(x_train):\n",
    "\n",
    "    # First we start with an input layer\n",
    "    inputs = Input(shape=x_train.shape[1:]) \n",
    "\n",
    "    normalizer = tf.keras.layers.Normalization()\n",
    "    normalizer.adapt(x_train)\n",
    "    layers = normalizer(inputs)\n",
    "\n",
    "    layers = Dropout(rate=DROP_RATE,\n",
    "                     seed=SEED)(layers) \n",
    "    \n",
    "    for hidden, activation, ridge in zip(HIDDENS, ACTIVATIONS, RIDGE):\n",
    "        layers = Dense(hidden, activation=activation,\n",
    "                       kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.00, l2=ridge),\n",
    "                       bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED),\n",
    "                       kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED))(layers)\n",
    "\n",
    "\n",
    "    # Output layer has a softmax function to convert output to class likelihood\n",
    "    output_layer = Dense(1, activation='linear',\n",
    "                      bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED),\n",
    "                      kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED))(layers)\n",
    "\n",
    "    # Constructing the model\n",
    "    model = Model(inputs, output_layer)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) # Using the Adam optimizer\n",
    "    model.compile(optimizer=optimizer, loss=LOSS, metrics=['mse',])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807abd7-832a-484b-98cd-7e6c3a9f60c0",
   "metadata": {
    "id": "c807abd7-832a-484b-98cd-7e6c3a9f60c0"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becb266-c9fd-4098-a2ba-e6c52804b8bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "executionInfo": {
     "elapsed": 105064,
     "status": "ok",
     "timestamp": 1646449809976,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "7becb266-c9fd-4098-a2ba-e6c52804b8bd",
    "outputId": "5f2d4b54-fb88-418f-95a2-3c5e281cc2e4"
   },
   "outputs": [],
   "source": [
    "SEED = 8889\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 1_000\n",
    "HIDDENS = [5,5]\n",
    "RIDGE = [.75, 0.]\n",
    "DROP_RATE = 0.0\n",
    "ACTIVATIONS = ['relu'] * len(HIDDENS) \n",
    "LOSS = 'mae'\n",
    "PATIENCE = 30\n",
    "VERBOSITY = 0\n",
    "#----------------------------------------\n",
    "\n",
    "tf.keras.backend.clear_session()            \n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# define early stopping callback\n",
    "EARLY_STOPPING = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                   patience=PATIENCE,\n",
    "                                                   verbose=1,\n",
    "                                                   mode='auto',\n",
    "                                                   restore_best_weights=True)\n",
    "\n",
    "\n",
    "model = compile_model(x_train)\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=NUM_EPOCHS, \n",
    "                    verbose=VERBOSITY,\n",
    "                    batch_size = BATCH_SIZE, \n",
    "                    shuffle=True,\n",
    "                    validation_data=[x_val, y_val],\n",
    "                    callbacks=[EARLY_STOPPING],\n",
    "                   )\n",
    "#----------------------------------------\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b05d0-da60-4eda-86b7-4327333093c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1646449810953,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "156b05d0-da60-4eda-86b7-4327333093c1",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "6d37c46a-c8a3-4e44-df23-edd7e9b38697",
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_train = model.predict(x_train)\n",
    "predict_val = model.predict(x_val)\n",
    "mae = np.mean(np.abs(predict_val[:,0]-y_val[:]))\n",
    "\n",
    "clr = ('tab:purple','tab:orange', 'tab:pink', 'tab:green', 'gold', 'violet','cornflowerblue','darkorange')\n",
    "#--------------------------------\n",
    "plt.subplots(1,2,figsize=(15,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(y_train, predict_train,'.',color='gray',alpha=.5, label='training')\n",
    "plt.plot(y_val, predict_val,'.', label='validation')\n",
    "plt.plot(y_val,y_val,'--',color='fuchsia')\n",
    "plt.axvline(x=0,color='gray',linewidth=1)\n",
    "plt.axhline(y=0,color='gray',linewidth=1)\n",
    "plt.title('Validation MAE = ' + str(mae.round(2)) + ' years')\n",
    "plt.xlabel('true number of years until target is reached')\n",
    "plt.ylabel('predicted number of years until target is reached')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(y_yrs_train, predict_train,'.',color='gray',alpha=.5, label='training')\n",
    "plt.xlabel('year of map')\n",
    "plt.ylabel('predicted number of years until target is reached')\n",
    "plt.axhline(y=0, color='gray', linewidth=1)\n",
    "\n",
    "predict_val_mat = predict_val.reshape(N_MODELS,N_VAL,len(YEARS_UNIQUE))\n",
    "for i in np.arange(0,predict_val_mat.shape[0]):\n",
    "    plt.plot(YEARS_UNIQUE, predict_val_mat[i,:,:].swapaxes(1,0),'.', label='validation', color=clr[i])\n",
    "    plt.axvline(x=target_years[i],linestyle='--',color=clr[i])\n",
    "if IN_COLAB==False:\n",
    "    pass\n",
    "    # plt.savefig('figures/initial_result_seed' + str(SEED) + '.png', dpi=savefig_dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ea61d-e514-415e-be5a-15b43367134f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Explainability via Input * Gradient and Integrated Gradients\n",
    "We will use two attribution explainaiblity methods called Input * Gradient and Integrated Gradients to make heatmaps of regions of the input that act as explanations for the network's prediction.\n",
    "\n",
    "* https://keras.io/examples/vision/integrated_gradients/\n",
    "* https://distill.pub/2020/attribution-baselines/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364bda5b-6cc2-4117-8820-44cdf07b4c60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gradients(inputs, top_pred_idx=None):\n",
    "    \"\"\"Computes the gradients of outputs w.r.t input image.\n",
    "\n",
    "    Args:\n",
    "        inputs: 2D/3D/4D matrix of samples\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.\n",
    "\n",
    "    Returns:\n",
    "        Gradients of the predictions w.r.t img_input\n",
    "    \"\"\"\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)\n",
    "        \n",
    "        # Run the forward pass of the layer and record operations\n",
    "        # on GradientTape.\n",
    "        preds = model(inputs, training=False)  \n",
    "        \n",
    "        # For classification, grab the top class\n",
    "        if top_pred_idx is not None:\n",
    "            preds = preds[:, top_pred_idx]\n",
    "        \n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.        \n",
    "    grads = tape.gradient(preds, inputs)\n",
    "    return grads\n",
    "\n",
    "def get_integrated_gradients(inputs, baseline=None, num_steps=50, top_pred_idx=None):\n",
    "    \"\"\"Computes Integrated Gradients for a prediction.\n",
    "\n",
    "    Args:\n",
    "        inputs (ndarray): 2D/3D/4D matrix of samples\n",
    "        baseline (ndarray): The baseline image to start with for interpolation\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.            \n",
    "\n",
    "    Returns:\n",
    "        Integrated gradients w.r.t input image\n",
    "    \"\"\"\n",
    "    # If baseline is not provided, start with zeros\n",
    "    # having same size as the input image.\n",
    "    if baseline is None:\n",
    "        input_size = np.shape(inputs)[1:]\n",
    "        baseline = np.zeros(input_size).astype(np.float32)\n",
    "    else:\n",
    "        baseline = baseline.astype(np.float32)\n",
    "\n",
    "    # 1. Do interpolation.\n",
    "    inputs = inputs.astype(np.float32)\n",
    "    interpolated_inputs = [\n",
    "        baseline + (step / num_steps) * (inputs - baseline)\n",
    "        for step in range(num_steps + 1)\n",
    "    ]\n",
    "    interpolated_inputs = np.array(interpolated_inputs).astype(np.float32)\n",
    "\n",
    "    # 3. Get the gradients\n",
    "    grads = []\n",
    "    for i, x_data in enumerate(interpolated_inputs):\n",
    "        grad = get_gradients(x_data, top_pred_idx=top_pred_idx)\n",
    "        grads.append(grad[0])\n",
    "    grads = tf.convert_to_tensor(grads, dtype=tf.float32)\n",
    "\n",
    "    # 4. Approximate the integral using the trapezoidal rule\n",
    "    grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "    avg_grads = tf.reduce_mean(grads, axis=0)\n",
    "\n",
    "    # 5. Calculate integrated gradients and return\n",
    "    integrated_grads = (inputs - baseline) * avg_grads\n",
    "    return integrated_grads\n",
    "\n",
    "def random_baseline_integrated_gradients(inputs, num_steps=50, num_runs=5, top_pred_idx=None):\n",
    "    \"\"\"Generates a number of random baseline images.\n",
    "\n",
    "    Args:\n",
    "        inputs (ndarray): 2D/3D/4D matrix of samples\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "        num_runs: number of baseline images to generate\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.      \n",
    "\n",
    "    Returns:\n",
    "        Averaged integrated gradients for `num_runs` baseline images\n",
    "    \"\"\"\n",
    "    # 1. List to keep track of Integrated Gradients (IG) for all the images\n",
    "    integrated_grads = []\n",
    "\n",
    "    # 2. Get the integrated gradients for all the baselines\n",
    "    for run in range(num_runs):\n",
    "        baseline = np.zeros(np.shape(inputs)[1:])\n",
    "        for i in np.arange(0,np.shape(random_baseline)[0]):\n",
    "            j = np.random.choice(np.arange(0,np.shape(inputs)[0]))\n",
    "            baseline[i] = inputs[j,i]\n",
    "\n",
    "        igrads = get_integrated_gradients(\n",
    "            inputs=inputs,\n",
    "            baseline=baseline,\n",
    "            num_steps=num_steps,\n",
    "        )\n",
    "        integrated_grads.append(igrads)\n",
    "\n",
    "    # 3. Return the average integrated gradients for the image\n",
    "    integrated_grads = tf.convert_to_tensor(integrated_grads)\n",
    "    return tf.reduce_mean(integrated_grads, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef431b-27ad-4874-959e-169e6f7f198c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#=========================================\n",
    "# Define the samples you want to explain\n",
    "rng = np.random.default_rng(45)\n",
    "isubsample = rng.choice(np.arange(0,x_val.shape[0]),\n",
    "                        size = 500,\n",
    "                        replace = False,\n",
    "                       )\n",
    "\n",
    "inputs = np.copy(x_val[isubsample,:])\n",
    "targets = np.copy(y_val[isubsample])\n",
    "yrs = np.copy(y_yrs_val[isubsample])\n",
    "preds = model.predict(inputs)\n",
    "#========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc382a-b5ce-4fca-a9a3-4cdae743276d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---------------------------------------\n",
    "# Gradient x Input\n",
    "#---------------------------------------\n",
    "# compute the multiplication of gradient * inputs\n",
    "# and reshape into a map of latitude x longitude\n",
    "grads = get_gradients(inputs).numpy()\n",
    "grad_x_input = grads * inputs\n",
    "grad_x_input = grad_x_input.reshape((len(targets),data_train.shape[2],data_train.shape[3]))\n",
    "print(np.shape(grad_x_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4cc98-6135-4fd8-9673-8f8af6a6e809",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---------------------------------------\n",
    "# Integrated Gradients\n",
    "#---------------------------------------\n",
    "BASELINE = 'zeros' #'zeros'#1850\n",
    "\n",
    "# Alternative baselines\n",
    "if(BASELINE=='zeros'):\n",
    "    baseline_mean = np.mean(x_train,axis=0)*0.    \n",
    "elif(BASELINE=='mean'):\n",
    "    baseline_mean = np.mean(x_train,axis=0)  \n",
    "else:\n",
    "    baseline_mean = np.mean(data_train,axis=0)\n",
    "    isample = np.where(years_train[0,:]==BASELINE)[0]\n",
    "    baseline_mean = np.mean(baseline_mean[isample,:,:],axis=0)\n",
    "    baseline_mean = baseline_mean.reshape((baseline_mean.shape[0]*baseline_mean.shape[1]))\n",
    "\n",
    "print('shape(baseline_mean) = ' + str(np.shape(baseline_mean)))\n",
    "print('model.predict(baseline_mean) = ' + str(model.predict(baseline_mean[np.newaxis,:])))\n",
    "\n",
    "igrad = get_integrated_gradients(inputs, baseline=baseline_mean)\n",
    "integrated_gradients = igrad.numpy().reshape((len(targets),data_train.shape[2],data_train.shape[3]))\n",
    "print(np.shape(integrated_gradients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c536cb-85e5-4b49-84ab-7de73aa82a25",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_map(x, clim=None, title=None, text=None, cmap='RdGy'):\n",
    "    if clim is None:\n",
    "        plt.pcolor(x,\n",
    "                   cmap=cmap,\n",
    "                   norm=mpl.colors.CenteredNorm(),        \n",
    "                  )\n",
    "    else:\n",
    "        plt.pcolor(x,\n",
    "                   cmap=cmap,\n",
    "                  )\n",
    "    plt.clim(clim)\n",
    "    plt.colorbar()\n",
    "    plt.title(title,fontsize=15,loc='right')    \n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.text(0.01, 1.0, text, fontfamily='monospace', fontsize='small', va='bottom',transform=plt.gca().transAxes)\n",
    "\n",
    "# plot the resulting heatmaps for a subset of samples\n",
    "# based on their label\n",
    "plot_list = (60, 40, 20, 10, 0)\n",
    "NCOL = 4\n",
    "plt.subplots(len(plot_list),NCOL,figsize=(35,5*len(plot_list)))\n",
    "\n",
    "for irow,min_range in enumerate(plot_list):\n",
    "        \n",
    "    max_range = min_range + 5\n",
    "    isamples = np.where((targets >= min_range) & (targets <= max_range))[0]\n",
    "    igrad_mean = np.mean(integrated_gradients[isamples,:,:],axis=0)\n",
    "    grad_x_input_mean = np.mean(grad_x_input[isamples,:,:],axis=0)\n",
    "    grad_mean = np.mean(grads[isamples,:],axis=0).reshape((data_train.shape[2],data_train.shape[3]))\n",
    "    x_inputs_mean = np.mean(inputs[isamples,:],axis=0).reshape((data_train.shape[2],data_train.shape[3]))\n",
    "    x_inputs_mean = x_inputs_mean - baseline_mean.reshape((data_train.shape[2],data_train.shape[3]))\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    text = (\n",
    "            \"\\n\"\n",
    "            + f\"  baseline       = {BASELINE}\\n\"           \n",
    "            + f\"  label_range    = {min_range}-{max_range} yrs.\\n\"                    \n",
    "            + f\"  n_samples      = {len(isamples)}\\n\"\n",
    "    )    \n",
    "    #------------------------------------------------------------------    \n",
    "    \n",
    "    # plot average input map\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+1)\n",
    "    plot_map(x_inputs_mean, \n",
    "             text=text,\n",
    "             # clim=(-10,10),\n",
    "             # clim=(250,310),\n",
    "             cmap='RdBu_r',\n",
    "             title = 'Temperature anomaly from Baseline',\n",
    "            )\n",
    "    #------------------------------------------------------------------\n",
    "    # plot explainability of gradient (saliency)\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+2)\n",
    "    plot_map(grad_mean, \n",
    "             text=text,             \n",
    "             clim=(-0.1, .1), \n",
    "             title = 'Gradient (Saliency)',\n",
    "            )\n",
    "    \n",
    "    #------------------------------------------------------------------\n",
    "    # plot explainability of input x gradient\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+3)\n",
    "    plot_map(grad_x_input_mean, \n",
    "             text=text,\n",
    "             clim=(-50,50),\n",
    "             title = 'Gradient x Input',\n",
    "            )\n",
    "\n",
    "    #------------------------------------------------------------------\n",
    "    # plot explainability of integrated gradients\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+4)\n",
    "    plot_map(igrad_mean, \n",
    "             text=text,             \n",
    "             # clim=(-.25,.25), \n",
    "             title = 'Integrated Gradients',\n",
    "            )\n",
    "\n",
    "plt.tight_layout()   \n",
    "if IN_COLAB==False:\n",
    "    pass\n",
    "    # plt.savefig('figures/xai_grid_' + str(min_range) +'-' + str(max_range) + '_baseline_' + str(BASELINE) + '.png', dpi=savefig_dpi)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ad419-afcc-4ab8-8a4a-d5cb3ca15c78",
   "metadata": {},
   "source": [
    "## Predict observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe28ad8-862a-401c-97c1-f8eb44f05038",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_obs = model.predict(x_obs)\n",
    "\n",
    "#--------------------------------\n",
    "i_year = np.where(y_predict_obs < 0)[0]\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(da_obs['time.year'], y_predict_obs, '.r')\n",
    "plt.xlabel('year of map')\n",
    "plt.ylabel('predicted number of years \\nuntil target is reached')\n",
    "plt.title('Observations Target Year = ' + str(da_obs['time.year'].values[i_year[0]]))\n",
    "plt.xlim(1850,2100)\n",
    "plt.ylim(-20,40)\n",
    "plt.axhline(y=0,color='gray')\n",
    "\n",
    "#--------------------------------\n",
    "plt.subplot(1,2,2)\n",
    "global_mean_obs.plot(linewidth=2,label='data',color=\"tab:orange\")\n",
    "plt.title('Observations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc649866-ed8f-476b-9e46-141e4d735b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "_main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
