{"cells":[{"cell_type":"markdown","id":"4a650402-4774-49cb-9b72-9c8f1dd02f1d","metadata":{"id":"4a650402-4774-49cb-9b72-9c8f1dd02f1d","tags":[]},"source":["# Detecting temperature targets\n","##### authors: Elizabeth A. Barnes and Noah Diffenbaugh\n","\n","##### README:\n","\n","* All user-defined parameters (other than plotting colors etc) are defined in the top cell called \"User-defined Experiment Parameters\". Most of your edits should just be there.\n","* The code will train either a typical regression problem ```reg``` or it will predict the conditional normal distribution ```shash2```\n","* The code provides the option of saving the tensorflow models (and their metadata) so that you can load them later and make predictions. This may be particularly useful if you find a good model that you like (and don't want to retrain over and over again) or if you want to save ~50 models and then use them to predict the observational fields to make a distribution of uncertainty (like I did in the google slides)\n","* The random seed calls should be written so that everything is exactly reproducible. So, run the same code tomorrow and you should get identical answers.\n"]},{"cell_type":"markdown","id":"9de167a6-1c3f-435f-815c-0ff078ef9d1e","metadata":{"id":"9de167a6-1c3f-435f-815c-0ff078ef9d1e"},"source":["## User-defined Experiment Parameters"]},{"cell_type":"code","execution_count":1,"id":"6454258d-b1e3-4815-952c-f68cd9435c6d","metadata":{"executionInfo":{"elapsed":163,"status":"ok","timestamp":1647291226116,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"},"user_tz":360},"id":"6454258d-b1e3-4815-952c-f68cd9435c6d","tags":[]},"outputs":[],"source":["exp_settings = {\n","    \"exp_name\": 'test',\n","    \"save_model\": True,\n","    \"n_models\": 1,                        # the number of networks you want to train\n","    \"ssp\" : \"370\",                        #[options: '126' or '370']\n","    \"target_temp\": 1.5,\n","    \"n_train_val_test\" : (7,3,0),\n","    \"baseline_yr_bounds\": (1850,1899),\n","    \"training_yr_bounds\": (1970,2100),\n","    \"anomaly_yr_bounds\": (1951,1980),\n","    \"anomalies\": True,                    #[options: True or False]\n","    \"remove_map_mean\": False,             #[options: False or \"weighted\" or \"raw\"]\n","    \"show_plots\" : False,                 #[options: False or True]\n","\n","    \"network_type\": 'reg',                #[options: \"reg\" or \"shash2\"]\n","    \"hiddens\": [10,10],\n","    \"dropout_rate\": 0.,\n","    \"ridge_param\": [1.,0.0], \n","    \"learning_rate\": 0.00001,\n","    \"batch_size\": 64,\n","    \"rng_seed\": 8889,\n","    \"act_fun\": [\"relu\",\"relu\"],\n","    \"n_epochs\": 25_000,\n","    \"patience\": 50,\n","    \"verbosity\": 0,\n","}"]},{"cell_type":"markdown","id":"482fc82c-3e30-4466-b896-1b292d5b2ef0","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"482fc82c-3e30-4466-b896-1b292d5b2ef0"},"source":["## Python setup"]},{"cell_type":"code","execution_count":2,"id":"1fdc0268-dd0a-4296-8a0b-f14b4c6189b2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161,"status":"ok","timestamp":1647291226117,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"},"user_tz":360},"id":"1fdc0268-dd0a-4296-8a0b-f14b4c6189b2","outputId":"fb097fc2-6ab8-45a5-ce73-ea03630bc3b8","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["IN_COLAB = True\n"]}],"source":["try:\n","    import google.colab\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False\n","print('IN_COLAB = ' + str(IN_COLAB))"]},{"cell_type":"code","execution_count":3,"id":"fb968382-4186-466e-a85b-b00caa5fc9be","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24221,"status":"ok","timestamp":1647291250184,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"},"user_tz":360},"id":"fb968382-4186-466e-a85b-b00caa5fc9be","outputId":"38d408f4-bfda-4226-a4d6-2c8e198cbfb5","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting xarray==0.20.2\n","  Downloading xarray-0.20.2-py3-none-any.whl (845 kB)\n","\u001b[?25l\r\u001b[K     |▍                               | 10 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 92 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 327 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 337 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 348 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 358 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 368 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 378 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 389 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 399 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 409 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 419 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 430 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 440 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 450 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 460 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 471 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 481 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 491 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 501 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 512 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 522 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 532 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 542 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 552 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 563 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 573 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 583 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 593 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 604 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 614 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 624 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 634 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 645 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 655 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 665 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 675 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 686 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 696 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 706 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 716 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 727 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 737 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 747 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 757 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 768 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 778 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 788 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 798 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 808 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 819 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 829 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 839 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 845 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.7/dist-packages (from xarray==0.20.2) (1.3.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from xarray==0.20.2) (4.11.2)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from xarray==0.20.2) (1.21.5)\n","Requirement already satisfied: typing-extensions>=3.7 in /usr/local/lib/python3.7/dist-packages (from xarray==0.20.2) (3.10.0.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1->xarray==0.20.2) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1->xarray==0.20.2) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1->xarray==0.20.2) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->xarray==0.20.2) (3.7.0)\n","Installing collected packages: xarray\n","  Attempting uninstall: xarray\n","    Found existing installation: xarray 0.18.2\n","    Uninstalling xarray-0.18.2:\n","      Successfully uninstalled xarray-0.18.2\n","Successfully installed xarray-0.20.2\n","Collecting nc-time-axis\n","  Downloading nc_time_axis-1.4.0-py3-none-any.whl (15 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from nc-time-axis) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nc-time-axis) (1.21.5)\n","Requirement already satisfied: cftime>=1.5 in /usr/local/lib/python3.7/dist-packages (from nc-time-axis) (1.6.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nc-time-axis) (3.0.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nc-time-axis) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nc-time-axis) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nc-time-axis) (1.3.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->nc-time-axis) (1.15.0)\n","Installing collected packages: nc-time-axis\n","Successfully installed nc-time-axis-1.4.0\n"]}],"source":["if IN_COLAB:\n","    !pip install xarray==0.20.2\n","    !pip install nc-time-axis\n","\n","import xarray as xr\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","import sys\n","\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Dense, Input, Dropout, Softmax\n","from tensorflow.keras import optimizers\n","from tensorflow.keras import regularizers\n","from tensorflow import keras\n","\n","import matplotlib as mpl\n","mpl.rcParams[\"figure.facecolor\"] = \"white\"\n","mpl.rcParams[\"figure.dpi\"] = 150\n","savefig_dpi = 300\n","np.warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"]},{"cell_type":"code","execution_count":4,"id":"29a5cee3-6f4f-4818-92e1-1351eeeb565a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74,"status":"ok","timestamp":1647291250186,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"},"user_tz":360},"id":"29a5cee3-6f4f-4818-92e1-1351eeeb565a","outputId":"8b2947e4-0985-404d-976d-8d6ee25795ff","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["python version = 3.7.12 (default, Jan 15 2022, 18:48:18) \n","[GCC 7.5.0]\n","numpy version = 1.21.5\n","xarray version = 0.20.2\n","tensorflow version = 2.8.0\n","tensorflow-probability version = 0.16.0\n"]}],"source":["print(f\"python version = {sys.version}\")\n","print(f\"numpy version = {np.__version__}\")\n","print(f\"xarray version = {xr.__version__}\")  \n","print(f\"tensorflow version = {tf.__version__}\")  \n","print(f\"tensorflow-probability version = {tfp.__version__}\")  "]},{"cell_type":"markdown","id":"d5d089c5-afe9-4d5c-93d1-b67662256c1a","metadata":{"tags":[],"id":"d5d089c5-afe9-4d5c-93d1-b67662256c1a"},"source":["## Data functions"]},{"cell_type":"code","execution_count":5,"id":"2e9edd00-2ab5-4f44-9bfa-751efe9f749b","metadata":{"id":"2e9edd00-2ab5-4f44-9bfa-751efe9f749b","executionInfo":{"status":"ok","timestamp":1647291250493,"user_tz":360,"elapsed":376,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"}}},"outputs":[],"source":["DATA_DIRECTORY = 'data/'\n","\n","def save_tf_model(model, model_name):\n","    \n","    import json\n","    \n","    if IN_COLAB:\n","        from google.colab import drive\n","        drive.mount('/content/drive', force_remount=True)\n","        model_path = './Colab Notebooks/'\n","        raise ValueError('@Noah - please set the GoogleDrive model_path above where you want to save things')\n","    else:\n","        MODEL_DIRECTORY = 'saved_models/'        \n","        \n","    tf.keras.models.save_model(\n","        model, MODEL_DIRECTORY + model_name + \"_model\", overwrite=True\n","    )\n","    with open(MODEL_DIRECTORY + model_name + '_metadata.json', 'w') as json_file:\n","        json_file.write(json.dumps(exp_settings))\n","\n","def get_netcdf_da(filename):\n","    if IN_COLAB:\n","        !pip install wget\n","        import wget\n","        nc_file = wget.download(\"https://eabarnes-data.atmos.colostate.edu/share/\" + filename)\n","    else:\n","        nc_file = DATA_DIRECTORY + filename\n","    \n","    da = xr.open_dataarray(nc_file)\n","    return da"]},{"cell_type":"code","execution_count":6,"id":"02eea0ec-ce97-469e-b7d6-da1fa0863927","metadata":{"tags":[],"id":"02eea0ec-ce97-469e-b7d6-da1fa0863927","executionInfo":{"status":"ok","timestamp":1647291250493,"user_tz":360,"elapsed":56,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"}}},"outputs":[],"source":["if exp_settings[\"ssp\"] == '370':\n","    filenames = ('tas_Amon_historical_ssp370_CanESM5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n","                 'tas_Amon_historical_ssp370_GISS-E2-1-G_r1-10_ncecat_ann_mean_2pt5degree.nc',\n","                 'tas_Amon_historical_ssp370_IPSL-CM6A-LR_r1-10_ncecat_ann_mean_2pt5degree.nc',\n","                 'tas_Amon_historical_ssp370_UKESM1-0-LL_r1-10_ncecat_ann_mean_2pt5degree.nc',\n","                 'tas_Amon_historical_ssp370_ACCESS-ESM1-5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n","                )\n","elif exp_settings[\"ssp\"] == '126':\n","    filenames = ('tas_Amon_historical_ssp126_CanESM5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n","                 'tas_Amon_historical_ssp126_MIROC6_r1-10_ncecat_ann_mean_2pt5degree.nc',\n","                 'tas_Amon_historical_ssp126_ACCESS-ESM1-5_r1-10_ncecat_ann_mean_2pt5degree.nc',\n","                 'tas_Amon_historical_ssp126_UKESM1-0-LL_r1-10_ncecat_ann_mean_2pt5degree.nc',\n","            )\n","else:\n","    raise NotImplementedError('no such SSP')\n","\n","N_GCMS = len(filenames)\n"]},{"cell_type":"markdown","id":"ca53d22e-95cb-4a2e-86b7-05dd68e5136e","metadata":{"id":"ca53d22e-95cb-4a2e-86b7-05dd68e5136e","tags":[]},"source":["### Define the target year, labels and pre-processing\n","\n","For starters, we will define the target year as the year that the ensemble mean, global mean temperature crosses the ```TARGET_TEMP``` threshold."]},{"cell_type":"code","execution_count":7,"id":"aae33333-64de-4bad-86c8-73b31083dacb","metadata":{"executionInfo":{"elapsed":361,"status":"ok","timestamp":1647291250799,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"},"user_tz":360},"id":"aae33333-64de-4bad-86c8-73b31083dacb","tags":[]},"outputs":[],"source":["def get_labels(da, plot=False):\n","    # compute the ensemble mean, global mean temperature\n","    # these computations should be based on the training set only\n","    da_ens = da.mean(axis=0)\n","    weights = np.cos(np.deg2rad(da_ens.lat))\n","    weights.name = \"weights\"\n","    temp_weighted = da_ens.weighted(weights)\n","    global_mean = temp_weighted.mean((\"lon\", \"lat\"))\n","    \n","    global_mean_ens = da.weighted(weights)\n","    global_mean_ens = global_mean_ens.mean((\"lon\",\"lat\"))\n","    \n","    # compute the target year \n","    baseline_mean = global_mean.sel(time=slice(str(exp_settings[\"baseline_yr_bounds\"][0]),str(exp_settings[\"baseline_yr_bounds\"][1]))).mean('time')\n","    iwarmer = np.where(global_mean.values > baseline_mean.values+exp_settings[\"target_temp\"])[0]\n","    target_year = global_mean[\"time\"].values[iwarmer[0]]\n","\n","    # plot the calculation to make sure things make sense\n","    if plot == True:\n","        for ens in np.arange(0,global_mean_ens.shape[0]):\n","            global_mean_ens[ens,:].plot(linewidth=1.0,color=\"gray\",alpha=.5)\n","        global_mean.plot(linewidth=2,label='data',color=\"aqua\")\n","        plt.axhline(y=baseline_mean, color='k', linestyle='-', label='baseline temp')\n","        plt.axhline(y=baseline_mean+exp_settings[\"target_temp\"], color='tab:blue',linewidth=1., linestyle='--', label='target temp')\n","        plt.axvline(x=target_year,color='tab:blue',linewidth=1., linestyle='--', label='target year')\n","        global_mean_obs.plot(linewidth=2,label='data',color=\"tab:orange\")        \n","        plt.xlabel('year')\n","        plt.ylabel('temp (K)')\n","        plt.title(f + '\\ntargets [' + str(target_year.year) + ', ' + str(exp_settings[\"target_temp\"]) + 'C]',\n","                  fontsize = 8,\n","                 )\n","        plt.show()\n","    \n","    # define the labels\n","    print('TARGET_YEAR = ' + str(target_year.year))\n","    labels = target_year.year - da['time.year'].values\n","    \n","    return labels, da['time.year'].values, target_year.year\n","\n","def preprocess_data(da, MEMBERS):\n","\n","    if MEMBERS is None:\n","        new_data = da\n","    else:\n","        new_data = da[MEMBERS,:,:,:]\n","\n","    if exp_settings[\"anomalies\"] is True:\n","        new_data = new_data - new_data.sel(time=slice(str(exp_settings[\"anomaly_yr_bounds\"][0]),str(exp_settings[\"anomaly_yr_bounds\"][1]))).mean('time')\n","        \n","    if exp_settings[\"remove_map_mean\"]  == 'raw':\n","        new_data = new_data - new_data.mean((\"lon\",\"lat\"))\n","    elif exp_settings[\"remove_map_mean\"] == 'weighted':\n","        weights = np.cos(np.deg2rad(new_data.lat))\n","        weights.name = \"weights\"\n","        new_data_weighted = new_data.weighted(weights)\n","        new_data = new_data - new_data_weighted.mean((\"lon\",\"lat\"))\n","\n","    return new_data\n","\n","def make_data_split(da, data, f_labels, f_years, labels, years, MEMBERS):\n","\n","    # process the data, i.e. compute anomalies, subtract the mean, etc.\n","    new_data = preprocess_data(da, MEMBERS)    \n","    \n","    # only train on certain samples\n","    iyears = np.where((f_years >= exp_settings[\"training_yr_bounds\"][0]) & (f_years <= exp_settings[\"training_yr_bounds\"][1]))[0]    \n","    f_years = f_years[iyears]\n","    f_labels = f_labels[iyears]            \n","    new_data = new_data[:,iyears,:,:]\n","    \n","    if data is None:\n","        data = new_data.values\n","        labels = np.tile(f_labels,(len(MEMBERS),1))        \n","        years = np.tile(f_years,(len(MEMBERS),1))\n","    else:\n","        data = np.concatenate((data,new_data.values),axis=0)        \n","        labels = np.concatenate((labels,np.tile(f_labels,(len(MEMBERS),1))),axis=0)        \n","        years = np.concatenate((years,np.tile(f_years,(len(MEMBERS),1))),axis=0)\n","    \n","    return data, labels, years"]},{"cell_type":"markdown","id":"4be3a1a6-74cf-44e3-abf6-2088cbea8e73","metadata":{"id":"4be3a1a6-74cf-44e3-abf6-2088cbea8e73"},"source":["### Load the data"]},{"cell_type":"markdown","id":"2485ac17-4b9d-460d-9486-29fb892143f4","metadata":{"id":"2485ac17-4b9d-460d-9486-29fb892143f4"},"source":["#### Load observations"]},{"cell_type":"code","execution_count":8,"id":"b01ce429-e5d7-4567-9bfe-3b19468fe5b4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b01ce429-e5d7-4567-9bfe-3b19468fe5b4","executionInfo":{"status":"ok","timestamp":1647291263898,"user_tz":360,"elapsed":13168,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"}},"outputId":"2715557f-87e7-46fb-c5a3-b05de56f6647"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=6548f0a9c922d92a9c0cbb6cb2fe9e625f8e9d39f084b94fe3d3ccd964e3b185\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n","np.shape(x_obs) = (172, 10368)\n"]}],"source":["nc_filename_obs = 'Land_and_Ocean_LatLong1_185001_202112_ann_mean_2pt5degree.nc'\n","da_obs = get_netcdf_da(nc_filename_obs)\n","weights = np.cos(np.deg2rad(da_obs.lat))\n","weights.name = \"weights\"\n","temp_weighted = da_obs.weighted(weights)\n","global_mean_obs = temp_weighted.mean((\"lon\", \"lat\"), skipna=False)\n","\n","data_obs = preprocess_data(da_obs, MEMBERS=None,) \n","x_obs = data_obs.values.reshape((data_obs.shape[0],data_obs.shape[1]*data_obs.shape[2]))\n","print('np.shape(x_obs) = ' + str(np.shape(x_obs)))"]},{"cell_type":"markdown","id":"2485df35-2e36-455e-9bbf-b412d0acc718","metadata":{"id":"2485df35-2e36-455e-9bbf-b412d0acc718"},"source":["#### Load CMIP"]},{"cell_type":"code","execution_count":9,"id":"f13226df-3764-402e-a6be-9e6ea3d3d6a5","metadata":{"id":"f13226df-3764-402e-a6be-9e6ea3d3d6a5","executionInfo":{"status":"ok","timestamp":1647291264025,"user_tz":360,"elapsed":174,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"}}},"outputs":[],"source":["N_TRAIN = exp_settings[\"n_train_val_test\"][0]\n","N_VAL   = exp_settings[\"n_train_val_test\"][1]\n","N_TEST  = exp_settings[\"n_train_val_test\"][2]\n","ALL_MEMBERS = np.arange(0,N_TRAIN+N_VAL+N_TEST)\n","\n","def get_cmip_data():\n","    data_train, data_val, data_test = None, None, None\n","    labels_train, labels_val, labels_test = None, None, None\n","    years_train, years_val, years_test = None, None, None\n","    target_years = []\n","\n","    for f in filenames:\n","        print(f)\n","        da = get_netcdf_da(f)\n","        f_labels, f_years, f_target_year = get_labels(da, plot=exp_settings[\"show_plots\"])\n","\n","        # create sets of train / validaton / test\n","        target_years = np.append(target_years,f_target_year)\n","        data_train, labels_train, years_train = make_data_split(da, \n","                                                                data_train, \n","                                                                f_labels, \n","                                                                f_years, \n","                                                                labels_train,\n","                                                                years_train,\n","                                                                train_members)\n","        data_val, labels_val, years_val       = make_data_split(da, \n","                                                                data_val, \n","                                                                f_labels, \n","                                                                f_years, \n","                                                                labels_val,\n","                                                                years_val,\n","                                                                val_members)\n","        data_test, labels_test, years_test    = make_data_split(da, \n","                                                                data_test, \n","                                                                f_labels, \n","                                                                f_years, \n","                                                                labels_test,\n","                                                                years_test,\n","                                                                test_members)\n","\n","    print('---------------------------')        \n","    YEARS_UNIQUE = np.unique(years_train)\n","    print('data_train.shape = ' + str(np.shape(data_train)))\n","    print('data_val.shape = ' + str(np.shape(data_val)))\n","    print('data_test.shape = ' + str(np.shape(data_test)))\n","    \n","    x_train = data_train.reshape((data_train.shape[0]*data_train.shape[1],data_train.shape[2]*data_train.shape[3]))\n","    x_val   = data_val.reshape((data_val.shape[0]*data_val.shape[1],data_val.shape[2]*data_val.shape[3]))\n","    x_test  = data_test.reshape((data_test.shape[0]*data_test.shape[1],data_test.shape[2]*data_test.shape[3]))\n","\n","    y_train = labels_train.reshape((data_train.shape[0]*data_train.shape[1],))\n","    y_val   = labels_val.reshape((data_val.shape[0]*data_val.shape[1],))\n","    y_test  = labels_test.reshape((data_test.shape[0]*data_test.shape[1],))\n","\n","    y_yrs_train = years_train.reshape((data_train.shape[0]*data_train.shape[1],))\n","    y_yrs_val   = years_val.reshape((data_val.shape[0]*data_val.shape[1],))\n","    y_yrs_test  = years_test.reshape((data_test.shape[0]*data_test.shape[1],))\n","\n","    print(x_train.shape, y_train.shape, y_yrs_train.shape)\n","    print(x_val.shape, y_val.shape, y_yrs_val.shape)\n","    print(x_test.shape, y_test.shape, y_yrs_test.shape)  \n","    \n","    map_shape = np.shape(data_train)[2:]\n","    \n","    return x_train, x_val, x_test, y_train, y_val, y_test, y_yrs_train, y_yrs_val, y_yrs_test, target_years, map_shape"]},{"cell_type":"markdown","id":"6bf95abe-3856-45d6-9204-6cb9bad3dfaf","metadata":{"id":"6bf95abe-3856-45d6-9204-6cb9bad3dfaf","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["## Network and XAI functions"]},{"cell_type":"code","execution_count":10,"id":"d2587a9c-7d17-4744-b4b2-f7781815ef2c","metadata":{"id":"d2587a9c-7d17-4744-b4b2-f7781815ef2c","executionInfo":{"status":"ok","timestamp":1647291264142,"user_tz":360,"elapsed":171,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"}}},"outputs":[],"source":["# define early stopping callback\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n","                                                   patience=exp_settings['patience'],\n","                                                   verbose=1,\n","                                                   mode='auto',\n","                                                   restore_best_weights=True)\n"]},{"cell_type":"code","execution_count":11,"id":"c947c8ff-72f2-40af-ade0-b97a37d1d962","metadata":{"tags":[],"id":"c947c8ff-72f2-40af-ade0-b97a37d1d962","executionInfo":{"status":"ok","timestamp":1647291265319,"user_tz":360,"elapsed":1205,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"}}},"outputs":[],"source":["class Exponentiate(keras.layers.Layer):\n","    \"\"\"Custom layer to exp the sigma and tau estimates inline.\"\"\"\n","\n","    def __init__(self, **kwargs):\n","        super(Exponentiate, self).__init__(**kwargs)\n","\n","    def call(self, inputs):\n","        return tf.math.exp(inputs)\n","\n","class InterquartileCapture(tf.keras.metrics.Metric):\n","    \"\"\"Compute the fraction of true values between the 25 and 75 percentiles.\n","\n","    \"\"\"\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n","        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, pred, sample_weight=None):\n","        mu = pred[:, 0]\n","        sigma = pred[:, 1]\n","\n","        if pred.shape[1] >= 3:\n","            gamma = pred[:, 2]\n","        else:\n","            gamma = tf.zeros_like(mu)\n","\n","        if pred.shape[1] >= 4:\n","            tau = pred[:, 3]\n","        else:\n","            tau = tf.ones_like(mu)\n","\n","        lower = shash_quantile(0.25, mu, sigma, gamma, tau)\n","        upper = shash_quantile(0.75, mu, sigma, gamma, tau)\n","\n","        batch_count = tf.reduce_sum(\n","            tf.cast(\n","                tf.math.logical_and(\n","                    tf.math.greater(y_true[:, 0], lower),\n","                    tf.math.less(y_true[:, 0], upper)\n","                ),\n","                tf.float32\n","            )\n","\n","        )\n","        batch_total = len(y_true[:, 0])\n","\n","        self.count.assign_add(tf.cast(batch_count, tf.float32))\n","        self.total.assign_add(tf.cast(batch_total, tf.float32))\n","\n","    def result(self):\n","        return self.count / self.total\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config}\n","\n","\n","class SignTest(tf.keras.metrics.Metric):\n","    \"\"\"Compute the fraction of true values above the median.\n","\n","    \"\"\"\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n","        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, pred, sample_weight=None):\n","        mu = pred[:, 0]\n","        sigma = pred[:, 1]\n","\n","        if pred.shape[1] >= 3:\n","            gamma = pred[:, 2]\n","        else:\n","            gamma = tf.zeros_like(mu)\n","\n","        if pred.shape[1] >= 4:\n","            tau = pred[:, 3]\n","        else:\n","            tau = tf.ones_like(mu)\n","\n","        median = shash_median(mu, sigma, gamma, tau)\n","\n","        batch_count = tf.reduce_sum(\n","            tf.cast(tf.math.greater(y_true[:, 0], median), tf.float32)\n","        )\n","        batch_total = len(y_true[:, 0])\n","\n","        self.count.assign_add(tf.cast(batch_count, tf.float32))\n","        self.total.assign_add(tf.cast(batch_total, tf.float32))\n","\n","    def result(self):\n","        return self.count / self.total\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config}\n","\n","    \n","class CustomMAE(tf.keras.metrics.Metric):\n","    \"\"\"Compute the prediction mean absolute error.\n","\n","    The \"predicted value\" is the median of the conditional distribution.\n","\n","    Notes\n","    -----\n","    * The computation is done by maintaining running sums of total predictions\n","        and correct predictions made across all batches in an epoch. The\n","        running sums are reset at the end of each epoch.\n","\n","    \"\"\"\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.error = self.add_weight(\"error\", initializer=\"zeros\")\n","        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, pred, sample_weight=None):\n","        mu = pred[:, 0]\n","        sigma = pred[:, 1]\n","\n","        if pred.shape[1] >= 3:\n","            gamma = pred[:, 2]\n","        else:\n","            gamma = tf.zeros_like(mu)\n","\n","        if pred.shape[1] >= 4:\n","            tau = pred[:, 3]\n","        else:\n","            tau = tf.ones_like(mu)\n","\n","        predictions = shash_median(mu, sigma, gamma, tau)\n","\n","        error = tf.math.abs(y_true[:, 0] - predictions)\n","        batch_error = tf.reduce_sum(error)\n","        batch_total = tf.math.count_nonzero(error)\n","\n","        self.error.assign_add(tf.cast(batch_error, tf.float32))\n","        self.total.assign_add(tf.cast(batch_total, tf.float32))\n","\n","    def result(self):\n","        return self.error / self.total\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config}\n","   \n","    \n","def compute_NLL(y, distr): \n","    return -distr.log_prob(y) \n","\n","def compute_shash_NLL(y_true, pred):\n","    \"\"\"Negative log-likelihood loss using the sinh-arcsinh normal distribution.\n","\n","    Arguments\n","    ---------\n","    y_true : tensor\n","        The ground truth values.\n","        shape = [batch_size, n_parameter]\n","\n","    pred :\n","        The predicted local conditionsal distribution parameter values.\n","        shape = [batch_size, n_parameters]\n","\n","    Returns\n","    -------\n","    loss : tensor, shape = [1, 1]\n","        The average negative log-likelihood of the batch using the predicted\n","        conditional distribution parameters.\n","\n","    Notes\n","    -----\n","    * The value of n_parameters depends on the chosen form of the conditional\n","        sinh-arcsinh normal distribution.\n","            shash2 -> n_parameter = 2, i.e. mu, sigma\n","            shash3 -> n_parameter = 3, i.e. mu, sigma, gamma\n","            shash4 -> n_parameter = 4, i.e. mu, sigma, gamma, tau\n","\n","    * Since sigma and tau must be strictly positive, the network learns the\n","        log of these two parameters.\n","\n","    * If gamma is not learned (i.e. shash2), they are set to 0.\n","\n","    * If tau is not learned (i.e. shash2 or shash3), they are set to 1.\n","\n","    \"\"\"\n","    mu = pred[:, 0]\n","    sigma = pred[:, 1]\n","\n","    if pred.shape[1] >= 3:\n","        gamma = pred[:, 2]\n","    else:\n","        gamma = tf.zeros_like(mu)\n","\n","    if pred.shape[1] >= 4:\n","        tau = pred[:, 3]\n","    else:\n","        tau = tf.ones_like(mu)\n","\n","    loss = -shash_log_prob(y_true[:, 0], mu, sigma, gamma, tau)\n","    return tf.reduce_mean(loss, axis=-1)\n","\n","\"\"\"sinh-arcsinh normal distribution w/o using tensorflow_probability.\n","\n","Functions\n","---------\n","cdf(x, mu, sigma, gamma, tau=None)\n","    cumulative distribution function (cdf).\n","\n","log_prob(x, mu, sigma, gamma, tau=None)\n","    log of the probability density function.\n","\n","mean(mu, sigma, gamma, tau=None)\n","    distribution mean.\n","\n","median(mu, sigma, gamma, tau=None)\n","    distribution median.\n","\n","prob(x, mu, sigma, gamma, tau=None)\n","    probability density function (pdf).\n","\n","quantile(pr, mu, sigma, gamma, tau=None)\n","    inverse cumulative distribution function.\n","\n","rvs(mu, sigma, gamma, tau=None, size=1)\n","    generate random variates.\n","\n","stddev(mu, sigma, gamma, tau=None)\n","    distribution standard deviation.\n","\n","variance(mu, sigma, gamma, tau=None)\n","    distribution variance.\n","\n","Notes\n","-----\n","* This module uses only tensorflow.  This module does not use the\n","tensorflow_probability library.\n","\n","* The sinh-arcsinh normal distribution was defined in [1]. A more accessible\n","presentation is given in [2]. \n","\n","* The notation and formulation used in this code was taken from [3], page 143.\n","In the gamlss.dist/CRAN package the distribution is called SHASHo. \n","\n","* There is a typographical error in the presentation of the probability \n","density function on page 143 of [3]. There is an extra \"2\" in the denomenator\n","preceeding the \"sqrt{1 + z^2}\" term.\n","\n","References\n","----------\n","[1] Jones, M. C. & Pewsey, A., Sinh-arcsinh distributions,\n","Biometrika, Oxford University Press, 2009, 96, 761-780.\n","DOI: 10.1093/biomet/asp053.\n","\n","[2] Jones, C. & Pewsey, A., The sinh-arcsinh normal distribution,\n","Significance, Wiley, 2019, 16, 6-7.\n","DOI: 10.1111/j.1740-9713.2019.01245.x.\n","https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2019.01245.x\n","\n","[3] Stasinopoulos, Mikis, et al. (2021), Distributions for Generalized \n","Additive Models for Location Scale and Shape, CRAN Package.\n","https://cran.r-project.org/web/packages/gamlss.dist/gamlss.dist.pdf\n","\n","\"\"\"\n","import numpy as np\n","import scipy\n","import scipy.stats\n","import tensorflow as tf\n","\n","__author__ = \"Randal J. Barnes and Elizabeth A. Barnes\"\n","__date__ = \"14 January 2022\"\n","\n","\n","SQRT_TWO = 1.4142135623730950488016887\n","ONE_OVER_SQRT_TWO = 0.7071067811865475244008444\n","TWO_PI = 6.2831853071795864769252868\n","SQRT_TWO_PI = 2.5066282746310005024157653\n","ONE_OVER_SQRT_TWO_PI = 0.3989422804014326779399461\n","\n","\n","def _jones_pewsey_P(q):\n","    \"\"\"P_q function from page 764 of [1].\n","\n","    Arguments\n","    ---------\n","    q : float, array like\n","\n","    Returns\n","    -------\n","    P_q : array like of same shape as q.\n","\n","    Notes\n","    -----\n","    * The formal equation is\n","\n","            jp = 0.25612601391340369863537463 * (\n","                scipy.special.kv((q + 1) / 2, 0.25) +\n","                scipy.special.kv((q - 1) / 2, 0.25)\n","            )\n","\n","        The strange constant 0.25612... is \"sqrt( sqrt(e) / (8*pi) )\" computed\n","        with a high-precision calculator.  The special function\n","\n","            scipy.special.kv\n","\n","        is the Modified Bessel function of the second kind: K(nu, x).\n","\n","    * But, we cannot use the scipy.special.kv function during tensorflow\n","        training.  This code uses a 6th order polynomial approximation in\n","        place of the formal function.\n","\n","    * This approximation is well behaved for 0 <= q <= 10. Since q = 1/tau\n","        or q = 2/tau in our applications, the approximation is well behaved\n","        for 1/10 <= tau < infty.\n","\n","    \"\"\"\n","    # A 6th order polynomial approximation of log(_jones_pewsey_P) for the\n","    # range 0 <= q <= 10.  Over this range, the max |error|/true < 0.0025.\n","    # These coefficients were computed by minimizing the maximum relative\n","    # error, and not by a simple least squares regression.\n","    coeffs = [\n","        9.37541380598926e-06,\n","        -0.000377732651131894,\n","        0.00642826706073389,\n","        -0.061281078712518,\n","        0.390956214318641,\n","        -0.0337884356755193,\n","        0.00248824801827172\n","    ]\n","    return tf.math.exp(tf.math.polyval(coeffs, q))\n","\n","\n","def shash_cdf(x, mu, sigma, gamma, tau=None):\n","    \"\"\"Cumulative distribution function (cdf).\n","\n","    Parameters\n","    ----------\n","    x : float (batch size x 1) Tensor\n","        The values at which to compute the probability density function.\n","\n","    mu : float (batch size x 1) Tensor\n","        The location parameter. Must be the same shape as x.\n","\n","    sigma : float (batch size x 1) Tensor\n","        The scale parameter. Must be strictly positive. Must be the same\n","        shape as x.\n","\n","    gamma : float (batch size x 1) Tensor\n","        The skewness parameter. Must be the same shape as x.\n","\n","    tau : float (batch size x 1) Tensor or None\n","        The tail-weight parameter. Must be strictly positive. Must be the same\n","        shape as x. If tau is None then the default value of tau=1 is used.\n","\n","    Returns\n","    -------\n","    F : float (batch size x 1) Tensor.\n","        The computed cumulative probability distribution function (cdf)\n","        evaluated at the values of x.  F has the same shape as x.\n","\n","    Notes\n","    -----\n","    * This function uses the tensorflow.math.erf function rather than the\n","    tensorflow_probability normal distribution functions.\n","\n","    \"\"\"\n","    y = (x - mu) / sigma    \n","    \n","    if tau is None:\n","        z = tf.math.sinh(tf.math.asinh(y) - gamma)\n","    else:\n","        z = tf.math.sinh(tau * tf.math.asinh(y) - gamma)\n","\n","    return 0.5 * (1.0 + tf.math.erf(ONE_OVER_SQRT_TWO * z))\n","\n","\n","def shash_log_prob(x, mu, sigma, gamma, tau=None):\n","    \"\"\"Log-probability density function.\n","\n","    Parameters\n","    ----------\n","    x : float (batch size x 1) Tensor\n","        The values at which to compute the probability density function.\n","\n","    mu : float (batch size x 1) Tensor\n","        The location parameter. Must be the same shape as x.\n","\n","    sigma : float (batch size x 1) Tensor\n","        The scale parameter. Must be strictly positive. Must be the same\n","        shape as x.\n","\n","    gamma : float (batch size x 1) Tensor\n","        The skewness parameter. Must be the same shape as x.\n","\n","    tau : float (batch size x 1) Tensor\n","        The tail-weight parameter. Must be strictly positive. Must be the same\n","        shape as x. If tau is None then the default value of tau=1 is used.\n","\n","    Returns\n","    -------\n","    f : float (batch size x 1) Tensor.\n","        The natural logarithm of the computed probability density function\n","        evaluated at the values of x.  f has the same shape as x.\n","\n","    Notes\n","    -----\n","    * This function is included merely to emulate the tensorflow_probability\n","    distributions.\n","\n","    \"\"\"\n","    return tf.math.log(shash_prob(x, mu, sigma, gamma, tau))\n","\n","\n","def shash_mean(mu, sigma, gamma, tau=None):\n","    \"\"\"The distribution mean.\n","\n","    Arguments\n","    ---------\n","    mu : float (batch size x 1) Tensor\n","        The location parameter.\n","\n","    sigma : float (batch size x 1) Tensor\n","        The scale parameter. Must be strictly positive. Must be the same\n","        shape as mu.\n","\n","    gamma : float (batch size x 1) Tensor\n","        The skewness parameter. Must be the same shape as mu.\n","\n","    tau : float (batch size x 1) Tensor\n","        The tail-weight parameter. Must be strictly positive. Must be the same\n","        shape as mu. If tau is None then the default value of tau=1 is used.\n","\n","    Returns\n","    -------\n","    x : float (batch size x 1) Tensor.\n","        The computed distribution mean values.\n","\n","    Notes\n","    -----\n","    * This equation for evX can be found on page 764 of [1].\n","\n","    \"\"\"\n","    if tau is None:\n","        evX = tf.math.sinh(gamma) * 1.35453080648132  \n","    else:\n","        evX = tf.math.sinh(gamma / tau) * _jones_pewsey_P(1.0 / tau)\n","\n","    return mu + sigma * evX\n","\n","\n","def shash_median(mu, sigma, gamma, tau=None):\n","    \"\"\"The distribution median.\n","\n","    Arguments\n","    ---------\n","    mu : float (batch size x 1) Tensor\n","        The location parameter.\n","\n","    sigma : float (batch size x 1) Tensor\n","        The scale parameter. Must be strictly positive. Must be the same\n","        shape as mu.\n","\n","    gamma : float (batch size x 1) Tensor\n","        The skewness parameter. Must be the same shape as mu.\n","\n","    tau : float (batch size x 1) Tensor\n","        The tail-weight parameter. Must be strictly positive. Must be the same\n","        shape as mu. If tau is None then the default value of tau=1 is used.\n","\n","    Returns\n","    -------\n","    x : float (batch size x 1) Tensor.\n","        The computed distribution mean values.\n","\n","    Notes\n","    -----\n","    * This code uses the basic formula:\n","\n","        E(a*X + b) = a*E(X) + b\n","\n","    * The E(X) is computed using the moment equation given on page 764 of [1].\n","\n","    \"\"\"\n","    if tau is None:\n","        return mu + sigma * tf.math.sinh(gamma)\n","    else:\n","        return mu + sigma * tf.math.sinh(gamma / tau)\n","\n","\n","def shash_prob(x, mu, sigma, gamma, tau=None):\n","    \"\"\"Probability density function (pdf).\n","\n","    Parameters\n","    ----------\n","    x : float (batch size x 1) Tensor\n","        The values at which to compute the probability density function.\n","\n","    mu : float (batch size x 1) Tensor\n","        The location parameter. Must be the same shape as x.\n","\n","    sigma : float (batch size x 1) Tensor\n","        The scale parameter. Must be strictly positive. Must be the same\n","        shape as x.\n","\n","    gamma : float (batch size x 1) Tensor\n","        The skewness parameter. Must be the same shape as x.\n","\n","    tau : float (batch size x 1) Tensor\n","        The tail-weight parameter. Must be strictly positive. Must be the same\n","        shape as x. If tau is None then the default value of tau=1 is used.\n","\n","    Returns\n","    -------\n","    f : float (batch size x 1) Tensor.\n","        The computed probability density function evaluated at the values of x.\n","        f has the same shape as x.\n","\n","    Notes\n","    -----\n","    * This code uses the equations on page 143 of [3], and the associated\n","    notation.\n","\n","    \"\"\"\n","    y = (x - mu) / sigma\n","    \n","    if tau is None:\n","        rsqr = tf.math.square(tf.math.sinh(tf.math.asinh(y) - gamma))\n","        return (\n","            ONE_OVER_SQRT_TWO_PI\n","            / sigma\n","            * tf.math.sqrt((1 + rsqr) / (1 + tf.math.square(y)))\n","            * tf.math.exp(-rsqr / 2)\n","        )\n","        \n","    else:\n","        rsqr = tf.math.square(tf.math.sinh(tau * tf.math.asinh(y) - gamma))\n","        return (\n","            ONE_OVER_SQRT_TWO_PI\n","            * (tau / sigma)\n","            * tf.math.sqrt((1 + rsqr) / (1 + tf.math.square(y)))\n","            * tf.math.exp(-rsqr / 2)\n","        )\n","\n","\n","def shash_quantile(pr, mu, sigma, gamma, tau=None):\n","    \"\"\"Inverse cumulative distribution function.\n","\n","    Arguments\n","    ---------\n","    pr : float (batch size x 1) Tensor.\n","        The probabilities at which to compute the values.\n","\n","    mu : float (batch size x 1) Tensor\n","        The location parameter. Must be the same shape as pr.\n","\n","    sigma : float (batch size x 1) Tensor\n","        The scale parameter. Must be strictly positive. Must be the same\n","        shape as pr.\n","\n","    gamma : float (batch size x 1) Tensor\n","        The skewness parameter. Must be the same shape as pr.\n","\n","    tau : float (batch size x 1) Tensor\n","        The tail-weight parameter. Must be strictly positive. Must be the same\n","        shape as pr. If tau is None then the default value of tau=1 is used.\n","\n","    Returns\n","    -------\n","    x : float (batch size x 1) Tensor.\n","        The computed values at the specified probabilities. f has the same\n","        shape as pr.\n","\n","    \"\"\"\n","    z = tf.math.ndtri(pr)\n","            \n","    if tau is None:\n","        return mu + sigma * tf.math.sinh(tf.math.asinh(z) + gamma)    \n","    else:\n","        return mu + sigma * tf.math.sinh((tf.math.asinh(z) + gamma) / tau)\n","\n","\n","def shash_rvs(mu, sigma, gamma, tau=None, size=1):\n","    \"\"\"Generate an array of random variates.\n","\n","    Arguments\n","    ---------\n","    mu : float or double scalar\n","        The location parameter.\n","\n","    sigma : float or double scalar\n","        The scale parameter. Must be strictly positive.\n","\n","    gamma : float or double scalar\n","        The skewness parameter.\n","\n","    tau : float or double scalar, or None\n","        The tail-weight parameter. Must be strictly positive. \n","        If tau is None then the default value of tau=1 is used.\n","\n","    size : int or tuple of ints, default=1.\n","        The number of random variates.\n","\n","    Returns\n","    -------\n","    x : double ndarray of size=size\n","        The generated random variates.\n","\n","    \"\"\"\n","    z = scipy.stats.norm.rvs(size=size)\n","    \n","    if tau is None:\n","        return mu + sigma * np.sinh(np.arcsinh(z) + gamma)\n","    else:\n","        return mu + sigma * np.sinh((np.arcsinh(z) + gamma) / tau)\n","\n","\n","def shash_stddev(mu, sigma, gamma, tau=None):\n","    \"\"\"The distribution standard deviation.\n","\n","    Arguments\n","    ---------\n","    mu : float (batch size x 1) Tensor\n","        The location parameter.\n","\n","    sigma : float (batch size x 1) Tensor\n","        The scale parameter. Must be strictly positive. Must be the same\n","        shape as mu.\n","\n","    gamma : float (batch size x 1) Tensor\n","        The skewness parameter. Must be the same shape as mu.\n","\n","    tau : float (batch size x 1) Tensor\n","        The tail-weight parameter. Must be strictly positive. Must be the same\n","        shape as mu. If tau is None then the default value of tau=1 is used.\n","\n","    Returns\n","    -------\n","    x : float (batch size x 1) Tensor.\n","        The computed distribution standard deviation values.\n","\n","    \"\"\"\n","    return tf.math.sqrt(variance(mu, sigma, gamma, tau))\n","\n","\n","def shash_variance(mu, sigma, gamma, tau=None):\n","    \"\"\"The distribution variance.\n","\n","    Arguments\n","    ---------\n","    mu : float (batch size x 1) Tensor\n","        The location parameter.\n","\n","    sigma : float (batch size x 1) Tensor\n","        The scale parameter. Must be strictly positive. Must be the same\n","        shape as mu.\n","\n","    gamma : float (batch size x 1) Tensor\n","        The skewness parameter. Must be the same shape as mu.\n","\n","    tau : float (batch size x 1) Tensor\n","        The tail-weight parameter. Must be strictly positive. Must be the same\n","        shape as mu. If tau is None then the default value of tau=1 is used.\n","\n","    Returns\n","    -------\n","    x : float (batch size x 1) Tensor.\n","        The computed distribution variance values.\n","\n","    Notes\n","    -----\n","    * This code uses two basic formulas:\n","\n","        var(X) = E(X^2) - (E(X))^2\n","        var(a*X + b) = a^2 * var(X)\n","\n","    * The E(X) and E(X^2) are computed using the moment equations given on\n","    page 764 of [1].\n","\n","    \"\"\"\n","    if tau is None:\n","        evX = tf.math.sinh(gamma) * 1.35453080648132\n","        evX2 = (tf.math.cosh(2 * gamma) * 3.0 - 1.0) / 2\n","    else:\n","        evX = tf.math.sinh(gamma / tau) * _jones_pewsey_P(1.0 / tau)\n","        evX2 = (tf.math.cosh(2 * gamma / tau) * _jones_pewsey_P(2.0 / tau) - 1.0) / 2\n","        \n","    return tf.math.square(sigma) * (evX2 - tf.math.square(evX))\n","\n","def percentile_value(mu_pred, sigma_pred, gamma_pred, tau_pred, percentile_frac=0.5):\n","    \"\"\"Function to obtain percentile value of the shash distribution.\"\"\"\n","    return shash_quantile(\n","        pr=percentile_frac, mu=mu_pred, sigma=sigma_pred, gamma=gamma_pred, tau=tau_pred\n","    ).numpy()"]},{"cell_type":"code","execution_count":12,"id":"7dffe81f-e03b-4fd1-bf6a-8afbfeedce72","metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1647291265675,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"},"user_tz":360},"id":"7dffe81f-e03b-4fd1-bf6a-8afbfeedce72","tags":[]},"outputs":[],"source":["   \n","def compile_model(x_train):\n","\n","    # First we start with an input layer\n","    inputs = Input(shape=x_train.shape[1:]) \n","\n","    normalizer = tf.keras.layers.Normalization()\n","    normalizer.adapt(x_train)\n","    layers = normalizer(inputs)\n","\n","    layers = Dropout(rate=exp_settings[\"dropout_rate\"],\n","                     seed=SEED)(layers) \n","    \n","    for hidden, activation, ridge in zip(exp_settings[\"hiddens\"], exp_settings[\"act_fun\"], exp_settings[\"ridge_param\"]):\n","        layers = Dense(hidden, activation=activation,\n","                       kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.00, l2=ridge),\n","                       bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED),\n","                       kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED))(layers)\n","\n","\n","    if exp_settings['network_type'] == 'reg':\n","        LOSS = 'mae'\n","        metrics = ['mse',]\n","        \n","        output_layer = Dense(1, activation='linear',\n","                          bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED),\n","                          kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED))(layers)\n","        \n","    elif exp_settings['network_type'] == 'shash2':\n","        LOSS = compute_shash_NLL\n","        metrics = [CustomMAE(name=\"custom_mae\"),\n","                    InterquartileCapture(name=\"interquartile_capture\"),\n","                    SignTest(name=\"sign_test\"),\n","                  ]\n","\n","        \n","        y_avg = np.mean(y_train)\n","        y_std = np.std(y_train)\n","\n","        mu_z_unit = tf.keras.layers.Dense(\n","            units=1,\n","            activation=\"linear\",\n","            use_bias=True,\n","            bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED+100),\n","            kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED+100),\n","            name=\"mu_z_unit\",\n","        )(layers)\n","        \n","        mu_unit = tf.keras.layers.Rescaling(\n","            scale=y_std,\n","            offset=y_avg,\n","            name=\"mu_unit\",\n","        )(mu_z_unit)\n","        \n","        # sigma_unit. The network predicts the log of the scaled sigma_z, then\n","        # the resclaing layer scales it up to log of sigma y, and the custom\n","        # Exponentiate layer converts it to sigma_y.\n","        log_sigma_z_unit = tf.keras.layers.Dense(\n","            units=1,\n","            activation=\"linear\",\n","            use_bias=True,\n","            bias_initializer=tf.keras.initializers.Zeros(),\n","            kernel_initializer=tf.keras.initializers.Zeros(),\n","            name=\"log_sigma_z_unit\",\n","        )(layers)\n","\n","        log_sigma_unit = tf.keras.layers.Rescaling(\n","            scale=1.0,\n","            offset=np.log(y_std),\n","            name=\"log_sigma_unit\",\n","        )(log_sigma_z_unit)\n","\n","        sigma_unit = Exponentiate(\n","            name=\"sigma_unit\",\n","        )(log_sigma_unit)\n","        \n","        output_layer = tf.keras.layers.concatenate([mu_unit, sigma_unit], axis=1)\n","        \n","    else:\n","        raise NotImpletementedError('no such network_type')\n","        \n","    # Constructing the model\n","    model = Model(inputs, output_layer)\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=exp_settings[\"learning_rate\"]), \n","                  loss=LOSS, \n","                  metrics=metrics,\n","                 )\n","        \n","        \n","    model.summary()\n","    \n","    return model\n"]},{"cell_type":"code","execution_count":13,"id":"3da52f05-ac5c-4ab5-9632-0bd82507dcc8","metadata":{"tags":[],"id":"3da52f05-ac5c-4ab5-9632-0bd82507dcc8","executionInfo":{"status":"ok","timestamp":1647291265877,"user_tz":360,"elapsed":232,"user":{"displayName":"Elizabeth Barnes","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64","userId":"07585723222468022011"}}},"outputs":[],"source":["def get_gradients(inputs, top_pred_idx=None):\n","    \"\"\"Computes the gradients of outputs w.r.t input image.\n","\n","    Args:\n","        inputs: 2D/3D/4D matrix of samples\n","        top_pred_idx: (optional) Predicted label for the x_data\n","                      if classification problem. If regression,\n","                      do not include.\n","\n","    Returns:\n","        Gradients of the predictions w.r.t img_input\n","    \"\"\"\n","    inputs = tf.cast(inputs, tf.float32)\n","\n","    with tf.GradientTape() as tape:\n","        tape.watch(inputs)\n","        \n","        # Run the forward pass of the layer and record operations\n","        # on GradientTape.\n","        preds = model(inputs, training=False)  \n","        \n","        # For classification, grab the top class\n","        if top_pred_idx is not None:\n","            preds = preds[:, top_pred_idx]\n","        \n","    # Use the gradient tape to automatically retrieve\n","    # the gradients of the trainable variables with respect to the loss.        \n","    grads = tape.gradient(preds, inputs)\n","    return grads\n","\n","def get_integrated_gradients(inputs, baseline=None, num_steps=50, top_pred_idx=None):\n","    \"\"\"Computes Integrated Gradients for a prediction.\n","\n","    Args:\n","        inputs (ndarray): 2D/3D/4D matrix of samples\n","        baseline (ndarray): The baseline image to start with for interpolation\n","        num_steps: Number of interpolation steps between the baseline\n","            and the input used in the computation of integrated gradients. These\n","            steps along determine the integral approximation error. By default,\n","            num_steps is set to 50.\n","        top_pred_idx: (optional) Predicted label for the x_data\n","                      if classification problem. If regression,\n","                      do not include.            \n","\n","    Returns:\n","        Integrated gradients w.r.t input image\n","    \"\"\"\n","    # If baseline is not provided, start with zeros\n","    # having same size as the input image.\n","    if baseline is None:\n","        input_size = np.shape(inputs)[1:]\n","        baseline = np.zeros(input_size).astype(np.float32)\n","    else:\n","        baseline = baseline.astype(np.float32)\n","\n","    # 1. Do interpolation.\n","    inputs = inputs.astype(np.float32)\n","    interpolated_inputs = [\n","        baseline + (step / num_steps) * (inputs - baseline)\n","        for step in range(num_steps + 1)\n","    ]\n","    interpolated_inputs = np.array(interpolated_inputs).astype(np.float32)\n","\n","    # 3. Get the gradients\n","    grads = []\n","    for i, x_data in enumerate(interpolated_inputs):\n","        grad = get_gradients(x_data, top_pred_idx=top_pred_idx)\n","        grads.append(grad[0])\n","    grads = tf.convert_to_tensor(grads, dtype=tf.float32)\n","\n","    # 4. Approximate the integral using the trapezoidal rule\n","    grads = (grads[:-1] + grads[1:]) / 2.0\n","    avg_grads = tf.reduce_mean(grads, axis=0)\n","\n","    # 5. Calculate integrated gradients and return\n","    integrated_grads = (inputs - baseline) * avg_grads\n","    return integrated_grads\n","\n","def random_baseline_integrated_gradients(inputs, num_steps=50, num_runs=5, top_pred_idx=None):\n","    \"\"\"Generates a number of random baseline images.\n","\n","    Args:\n","        inputs (ndarray): 2D/3D/4D matrix of samples\n","        num_steps: Number of interpolation steps between the baseline\n","            and the input used in the computation of integrated gradients. These\n","            steps along determine the integral approximation error. By default,\n","            num_steps is set to 50.\n","        num_runs: number of baseline images to generate\n","        top_pred_idx: (optional) Predicted label for the x_data\n","                      if classification problem. If regression,\n","                      do not include.      \n","\n","    Returns:\n","        Averaged integrated gradients for `num_runs` baseline images\n","    \"\"\"\n","    # 1. List to keep track of Integrated Gradients (IG) for all the images\n","    integrated_grads = []\n","\n","    # 2. Get the integrated gradients for all the baselines\n","    for run in range(num_runs):\n","        baseline = np.zeros(np.shape(inputs)[1:])\n","        for i in np.arange(0,np.shape(random_baseline)[0]):\n","            j = np.random.choice(np.arange(0,np.shape(inputs)[0]))\n","            baseline[i] = inputs[j,i]\n","\n","        igrads = get_integrated_gradients(\n","            inputs=inputs,\n","            baseline=baseline,\n","            num_steps=num_steps,\n","        )\n","        integrated_grads.append(igrads)\n","\n","    # 3. Return the average integrated gradients for the image\n","    integrated_grads = tf.convert_to_tensor(integrated_grads)\n","    return tf.reduce_mean(integrated_grads, axis=0)\n","\n"]},{"cell_type":"markdown","id":"c807abd7-832a-484b-98cd-7e6c3a9f60c0","metadata":{"id":"c807abd7-832a-484b-98cd-7e6c3a9f60c0","tags":[]},"source":["## Train the network"]},{"cell_type":"code","execution_count":null,"id":"7becb266-c9fd-4098-a2ba-e6c52804b8bd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7becb266-c9fd-4098-a2ba-e6c52804b8bd","outputId":"12e91837-29bd-4fc1-c3f7-b8c5555c6f5f","tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["[9 0 7 5 1 6 3] [8 4 2] []\n","tas_Amon_historical_ssp370_CanESM5_r1-10_ncecat_ann_mean_2pt5degree.nc\n","Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n","TARGET_YEAR = 2011\n","tas_Amon_historical_ssp370_GISS-E2-1-G_r1-10_ncecat_ann_mean_2pt5degree.nc\n","Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n","TARGET_YEAR = 2029\n","tas_Amon_historical_ssp370_IPSL-CM6A-LR_r1-10_ncecat_ann_mean_2pt5degree.nc\n","Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n","TARGET_YEAR = 2020\n","tas_Amon_historical_ssp370_UKESM1-0-LL_r1-10_ncecat_ann_mean_2pt5degree.nc\n","Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n","TARGET_YEAR = 2024\n","tas_Amon_historical_ssp370_ACCESS-ESM1-5_r1-10_ncecat_ann_mean_2pt5degree.nc\n","Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n","TARGET_YEAR = 2035\n","---------------------------\n","data_train.shape = (35, 131, 72, 144)\n","data_val.shape = (15, 131, 72, 144)\n","data_test.shape = (0, 131, 72, 144)\n","(4585, 10368) (4585,) (4585,)\n","(1965, 10368) (1965,) (1965,)\n","(0, 10368) (0,) (0,)\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 10368)]           0         \n","                                                                 \n"," normalization (Normalizatio  (None, 10368)            20737     \n"," n)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 10368)             0         \n","                                                                 \n"," dense (Dense)               (None, 10)                103690    \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                110       \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 11        \n","                                                                 \n","=================================================================\n","Total params: 124,548\n","Trainable params: 103,811\n","Non-trainable params: 20,737\n","_________________________________________________________________\n"]}],"source":["rng = np.random.default_rng(exp_settings[\"rng_seed\"])\n","\n","for iloop in np.arange(exp_settings['n_models']):\n","    SEED = rng.integers(low=1_000,high=10_000,size=1)[0]    \n","    tf.random.set_seed(SEED)\n","    np.random.seed(SEED)\n","\n","    train_members = rng.choice(ALL_MEMBERS, size=N_TRAIN, replace=False)\n","    val_members   = rng.choice(np.setdiff1d(ALL_MEMBERS,train_members), size=N_VAL, replace=False)\n","    test_members  = rng.choice(np.setdiff1d(ALL_MEMBERS,np.append(train_members[:],val_members)), size=N_TEST, replace=False)\n","    print(train_members, val_members, test_members)\n","\n","    x_train, x_val, x_test, y_train, y_val, y_test, y_yrs_train, y_yrs_val, y_yrs_test, target_years, map_shape = get_cmip_data()\n","\n","    tf.keras.backend.clear_session()            \n","    \n","#     # make onehot vectors for training\n","    if exp_settings[\"network_type\"] == 'shash2':\n","        onehot_train = np.zeros((x_train.shape[0],2))\n","        onehot_train[:,0] = y_train.astype('float32')\n","        onehot_val = np.zeros((x_val.shape[0],2))    \n","        onehot_val[:,0] = y_val.astype('float32')\n","        onehot_test = np.zeros((x_test.shape[0],2))    \n","        onehot_test[:,0] = y_test.astype('float32')\n","    else:\n","        onehot_train = np.copy(y_train)\n","        onehot_val = np.copy(y_val)\n","        onehot_test = np.copy(y_test)\n","        \n","    model = compile_model(x_train)\n","    history = model.fit(x_train, onehot_train, \n","                        epochs=exp_settings['n_epochs'], \n","                        verbose=exp_settings['verbosity'],\n","                        batch_size = exp_settings['batch_size'], \n","                        shuffle=True,\n","                        validation_data=[x_val, onehot_val],\n","                        callbacks=[early_stopping,],\n","                       )\n","    #----------------------------------------\n","    # save the tensorflow model\n","    model_name = (exp_settings[\"exp_name\"] + '_' + \n","                  exp_settings[\"network_type\"] + \n","                  '_rng' + str(exp_settings[\"rng_seed\"]) + \n","                  '_seed' + str(SEED)\n","                 )\n","    if exp_settings[\"save_model\"]:\n","        save_tf_model(model, model_name)\n","    \n","    #----------------------------------------\n","    if exp_settings[\"show_plots\"]:\n","        plt.plot(history.history['loss'], label='loss')\n","        plt.plot(history.history['val_loss'], label='val_loss')\n","        plt.xlabel('epoch')\n","        plt.ylabel('loss')\n","        plt.legend()\n","        plt.show()\n","        \n","        "]},{"cell_type":"markdown","id":"c635d170-ee55-4c82-8519-93b2565fcd5f","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"c635d170-ee55-4c82-8519-93b2565fcd5f"},"source":["## Make loss plots following training"]},{"cell_type":"code","execution_count":null,"id":"67a0f89f-a5b7-4e32-bae4-4f0f9efa73b3","metadata":{"id":"67a0f89f-a5b7-4e32-bae4-4f0f9efa73b3"},"outputs":[],"source":["def plot_metrics(history,metric):\n","    \n","    imin = np.argmin(history.history['val_loss'])\n","    \n","    plt.plot(history.history[metric], label='training')\n","    plt.plot(history.history['val_' + metric], label='validation')\n","    plt.title(metric)\n","    plt.axvline(x=imin, linewidth=.5, color='gray',alpha=.5)\n","    plt.legend()\n","\n","if exp_settings[\"network_type\"] == 'shash2':\n","    imin = len(history.history['custom_mae'])#np.argmin(history.history['val_loss'])\n","\n","    plt.subplots(figsize=(20,4))\n","    plt.subplot(1,4,1)\n","    plot_metrics(history,'loss')\n","    plt.ylim(0,10.)\n","\n","    plt.subplot(1,4,2)\n","    plot_metrics(history,'custom_mae')\n","    plt.ylim(0,10)\n","\n","    plt.subplot(1,4,3)\n","    plot_metrics(history,'interquartile_capture')\n","\n","    plt.subplot(1,4,4)\n","    plot_metrics(history,'sign_test')\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"647acbe8-c4f3-428a-95ca-e40373ec1af6","metadata":{"id":"647acbe8-c4f3-428a-95ca-e40373ec1af6"},"outputs":[],"source":["if exp_settings['network_type'] == \"shash2\":\n","    top_pred_idx = 0\n","else:\n","    top_pred_idx = None\n","    \n","YEARS_UNIQUE = np.unique(y_yrs_train)\n","predict_train = model.predict(x_train)[:,top_pred_idx]\n","predict_val = model.predict(x_val)[:,top_pred_idx]\n","mae = np.mean(np.abs(predict_val-y_val[:]))"]},{"cell_type":"code","execution_count":null,"id":"156b05d0-da60-4eda-86b7-4327333093c1","metadata":{"id":"156b05d0-da60-4eda-86b7-4327333093c1","tags":[]},"outputs":[],"source":["clr = ('tab:purple','tab:orange', 'tab:pink', 'tab:green', 'gold', 'violet','cornflowerblue','darkorange')\n","#--------------------------------\n","plt.subplots(1,2,figsize=(15,6))\n","\n","plt.subplot(1,2,1)\n","plt.plot(y_train, predict_train,'.',color='gray',alpha=.5, label='training')\n","plt.plot(y_val, predict_val,'.', label='validation')\n","plt.plot(y_val,y_val,'--',color='fuchsia')\n","plt.axvline(x=0,color='gray',linewidth=1)\n","plt.axhline(y=0,color='gray',linewidth=1)\n","plt.title('Validation MAE = ' + str(mae.round(2)) + ' years')\n","plt.xlabel('true number of years until target is reached')\n","plt.ylabel('predicted number of years until target is reached')\n","plt.legend()\n","\n","\n","plt.subplot(1,2,2)\n","plt.plot(y_yrs_train, predict_train,'.',color='gray',alpha=.5, label='training')\n","plt.xlabel('year of map')\n","plt.ylabel('predicted number of years until target is reached')\n","plt.axhline(y=0, color='gray', linewidth=1)\n","\n","predict_val_mat = predict_val.reshape(N_GCMS,N_VAL,len(YEARS_UNIQUE))\n","for i in np.arange(0,predict_val_mat.shape[0]):\n","    plt.plot(YEARS_UNIQUE, predict_val_mat[i,:,:].swapaxes(1,0),'.', label='validation', color=clr[i])\n","    plt.axvline(x=target_years[i],linestyle='--',color=clr[i])\n","if IN_COLAB==False:\n","    pass\n","    # plt.savefig('figures/initial_result_seed' + str(SEED) + '.png', dpi=savefig_dpi)\n","plt.show()"]},{"cell_type":"markdown","id":"272ea61d-e514-415e-be5a-15b43367134f","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"272ea61d-e514-415e-be5a-15b43367134f"},"source":["## Explainability via Input * Gradient and Integrated Gradients\n","We will use two attribution explainaiblity methods called Input * Gradient and Integrated Gradients to make heatmaps of regions of the input that act as explanations for the network's prediction.\n","\n","* https://keras.io/examples/vision/integrated_gradients/\n","* https://distill.pub/2020/attribution-baselines/"]},{"cell_type":"code","execution_count":null,"id":"364bda5b-6cc2-4117-8820-44cdf07b4c60","metadata":{"tags":[],"id":"364bda5b-6cc2-4117-8820-44cdf07b4c60"},"outputs":[],"source":["def plot_map(x, clim=None, title=None, text=None, cmap='RdGy'):\n","    plt.pcolor(x,\n","               cmap=cmap,\n","              )\n","    plt.clim(clim)\n","    plt.colorbar()\n","    plt.title(title,fontsize=15,loc='right')    \n","    plt.yticks([])\n","    plt.xticks([])\n","    \n","    plt.text(0.01, 1.0, text, fontfamily='monospace', fontsize='small', va='bottom',transform=plt.gca().transAxes)"]},{"cell_type":"code","execution_count":null,"id":"c6ef431b-27ad-4874-959e-169e6f7f198c","metadata":{"tags":[],"id":"c6ef431b-27ad-4874-959e-169e6f7f198c"},"outputs":[],"source":["#=========================================\n","# Define the samples you want to explain\n","rng = np.random.default_rng(45)\n","isubsample = rng.choice(np.arange(0,x_val.shape[0]),\n","                        size = 500,\n","                        replace = False,\n","                       )\n","\n","inputs = np.copy(x_val[isubsample,:])\n","targets = np.copy(y_val[isubsample])\n","yrs = np.copy(y_yrs_val[isubsample])\n","preds = model.predict(inputs)\n","\n","#=========================================\n","#---------------------------------------\n","# Gradient x Input\n","#---------------------------------------\n","# compute the multiplication of gradient * inputs\n","# and reshape into a map of latitude x longitude\n","\n","grads = get_gradients(inputs,top_pred_idx).numpy()\n","grad_x_input = grads * inputs\n","grad_x_input = grad_x_input.reshape((len(targets),map_shape[0],map_shape[1]))\n","print(np.shape(grad_x_input))\n","\n","#---------------------------------------\n","# Integrated Gradients\n","#---------------------------------------\n","baseline_mean = np.mean(x_train,axis=0)*0.    \n","print('shape(baseline_mean) = ' + str(np.shape(baseline_mean)))\n","print('model.predict(baseline_mean) = ' + str(model.predict(baseline_mean[np.newaxis,:])))\n","\n","igrad = get_integrated_gradients(inputs, baseline=baseline_mean,top_pred_idx=top_pred_idx)\n","integrated_gradients = igrad.numpy().reshape((len(targets),map_shape[0],map_shape[1]))\n","print(np.shape(integrated_gradients))"]},{"cell_type":"code","execution_count":null,"id":"27c536cb-85e5-4b49-84ab-7de73aa82a25","metadata":{"tags":[],"id":"27c536cb-85e5-4b49-84ab-7de73aa82a25"},"outputs":[],"source":["\n","# plot the resulting heatmaps for a subset of samples\n","# based on their label\n","plot_list = (60, 40, 20, 10, 0)\n","NCOL = 4\n","plt.subplots(len(plot_list),NCOL,figsize=(35,5*len(plot_list)))\n","\n","for irow,min_range in enumerate(plot_list):\n","        \n","    max_range = min_range + 5\n","    isamples = np.where((targets >= min_range) & (targets <= max_range))[0]\n","    igrad_mean = np.mean(integrated_gradients[isamples,:,:],axis=0)\n","    grad_x_input_mean = np.mean(grad_x_input[isamples,:,:],axis=0)\n","    grad_mean = np.mean(grads[isamples,:],axis=0).reshape((map_shape[0],map_shape[1]))\n","    x_inputs_mean = np.mean(inputs[isamples,:],axis=0).reshape((map_shape[0],map_shape[1]))\n","    x_inputs_mean = x_inputs_mean - baseline_mean.reshape((map_shape[0],map_shape[1]))\n","    #------------------------------------------------------------------\n","    \n","    text = (\n","            \"\\n\"\n","            + f\"  label_range    = {min_range}-{max_range} yrs.\\n\"                    \n","            + f\"  n_samples      = {len(isamples)}\\n\"\n","    )    \n","    #------------------------------------------------------------------    \n","    \n","    # plot average input map\n","    plt.subplot(len(plot_list),NCOL,irow*NCOL+1)\n","    plot_map(x_inputs_mean, \n","             text=text,\n","             clim=(-5,5),\n","             cmap='RdBu_r',\n","             title = 'Temperature anomaly from Baseline',\n","            )\n","    #------------------------------------------------------------------\n","    # plot explainability of gradient (saliency)\n","    plt.subplot(len(plot_list),NCOL,irow*NCOL+2)\n","    plot_map(grad_mean, \n","             text=text,             \n","             clim=(-0.02, .02), \n","             title = 'Gradient (Saliency)',\n","            )\n","    \n","    #------------------------------------------------------------------\n","    # plot explainability of input x gradient\n","    plt.subplot(len(plot_list),NCOL,irow*NCOL+3)\n","    plot_map(grad_x_input_mean, \n","             text=text,\n","             clim=(-.02,.02),\n","             title = 'Gradient x Input',\n","            )\n","\n","    #------------------------------------------------------------------\n","    # plot explainability of integrated gradients\n","    plt.subplot(len(plot_list),NCOL,irow*NCOL+4)\n","    plot_map(igrad_mean, \n","             text=text,             \n","             clim=(-.02,.02), \n","             title = 'Integrated Gradients',\n","            )\n","\n","plt.tight_layout()   \n","if IN_COLAB==False:\n","    pass\n","    # plt.savefig('figures/xai_grid_' + str(min_range) +'-' + str(max_range) + '_baseline_' + str(BASELINE) + '.png', dpi=savefig_dpi)\n","plt.show()\n","\n"]},{"cell_type":"markdown","id":"f25ad419-afcc-4ab8-8a4a-d5cb3ca15c78","metadata":{"tags":[],"id":"f25ad419-afcc-4ab8-8a4a-d5cb3ca15c78"},"source":["## Predict observations"]},{"cell_type":"code","execution_count":null,"id":"7fe28ad8-862a-401c-97c1-f8eb44f05038","metadata":{"tags":[],"id":"7fe28ad8-862a-401c-97c1-f8eb44f05038"},"outputs":[],"source":["import scipy.stats as stats\n","\n","if exp_settings[\"anomalies\"]:\n","    x_obs_predict = np.nan_to_num(x_obs,0.)\n","y_predict_obs = model.predict(x_obs_predict)[:,top_pred_idx]\n","\n","iy = np.where(da_obs['time.year'].values >= 2011)[0]\n","x = da_obs['time.year'].values[iy]\n","y = y_predict_obs[iy]\n","linear_model = stats.linregress(x=x,y=y)\n","\n","#--------------------------------\n","i_year = np.where(y_predict_obs < 0)[0]\n","plt.figure(figsize=(10,3))\n","plt.subplot(1,2,1)\n","plt.plot(da_obs['time.year'], y_predict_obs, '.r')\n","plt.plot(x, linear_model.slope*x+linear_model.intercept, '--k', alpha=.5, linewidth=2)\n","plt.xlabel('year of map')\n","plt.ylabel('predicted number of years \\nuntil target is reached')\n","plt.title('Observations Target Year = ' + \n","          str(np.round(2021+y_predict_obs[-1],1)) +\n","          ' (' + str(y_predict_obs[-1].round()) + ' years)'+\n","          '\\n slope = ' + str(linear_model.slope.round(2)) \n","         )\n","plt.xlim(1970,2025)\n","# plt.ylim(-20,40)\n","plt.axhline(y=0,color='gray')\n","\n","#--------------------------------\n","plt.subplot(1,2,2)\n","global_mean_obs.plot(linewidth=2,label='data',color=\"tab:orange\")\n","plt.title('Observations')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"310e3dd0-7bd4-4271-8af1-850cb1aee64e","metadata":{"id":"310e3dd0-7bd4-4271-8af1-850cb1aee64e"},"outputs":[],"source":["grads_obs = get_gradients(x_obs[-1:,:],top_pred_idx=top_pred_idx).numpy()*x_obs[-1:,:]\n","print('np.shape(grads_obs) = ' + str(np.shape(grads_obs)))\n","grads_obs_mean = np.mean(grads_obs,axis=0)\n","plt.figure(figsize=(10,4))\n","plot_map(grads_obs_mean.reshape((map_shape[0],map_shape[1])), \n","         clim = (-.02,.02),\n","         title = 'Observations - Gradient x Input',\n","        )"]},{"cell_type":"code","execution_count":null,"id":"2f952d05-2b3d-4705-91a9-1deb752591fa","metadata":{"id":"2f952d05-2b3d-4705-91a9-1deb752591fa"},"outputs":[],"source":["import scipy.stats as stats\n","import seaborn as sns\n","\n","if exp_settings[\"network_type\"] == 'shash2':\n","\n","    clr_choice = 'orange'\n","    y_predict_obs = model.predict(x_obs_predict)\n","\n","    iy = np.where(da_obs['time.year'].values >= 2011)[0]\n","    x = da_obs['time.year'].values[iy]\n","    y = y_predict_obs[iy,0]\n","    linear_model = stats.linregress(x=x,y=y)\n","\n","    #--------------------------------\n","    shash_incs = np.arange(-80,80,1)\n","    mu_pred = y_predict_obs[:,0]\n","    sigma_pred = y_predict_obs[:,1]\n","    gamma_pred = np.zeros(mu_pred.shape) #y_predict_obs[:,2]#np.zeros(mu_pred.shape)\n","    tau_pred = np.ones(mu_pred.shape)\n","    shash_perc_low = percentile_value(mu_pred, sigma_pred, gamma_pred, tau_pred, percentile_frac=.25)   \n","    shash_perc_high = percentile_value(mu_pred, sigma_pred, gamma_pred, tau_pred, percentile_frac=.75)\n","    shash_perc_med = percentile_value(mu_pred, sigma_pred, gamma_pred, tau_pred, percentile_frac=.5)\n","    shash_cpd = shash_prob(shash_incs,mu_pred[-1],sigma_pred[-1],gamma_pred[-1],tau_pred[-1])\n","\n","    print('2021 prediction = ' + str(mu_pred[-1]) + ' (' + str(shash_perc_low[-1]) + ' to ' + str(shash_perc_high[-1]) + ')')\n","\n","    y_predict_obs = mu_pred #shash_perc_med #shash_mean(mu_pred, sigma_pred, gamma_pred, tau_pred)#y_predict_obs[:,0]\n","\n","    #------------------------------------------------------------\n","    ax = plt.subplots(1,2,figsize=(16,4))\n","    years = np.arange(1850,2022)\n","\n","    plt.subplot(1,2,1)\n","    for iyear in np.arange(0,y_predict_obs.shape[0]):\n","        min_val = shash_perc_low[iyear]\n","        max_val = shash_perc_high[iyear]\n","\n","        if(years[iyear]==2021):\n","            clr = clr_choice\n","        else:\n","            clr = 'gray'\n","        plt.plot((years[iyear],years[iyear]),(min_val, max_val),\n","                 linestyle='-',\n","                 linewidth=4,\n","                 color=clr,\n","                )\n","\n","    plt.plot(x,x*linear_model.slope+linear_model.intercept,'--', color='black')\n","\n","    plt.xlim(1977.5,2023)    \n","    plt.ylim(0,57)\n","    plt.ylabel('years until target')\n","    plt.xlabel('year')\n","    plt.title('Observations predictions under SSP337 (shash)\\n slope=' + str(linear_model.slope.round(2)))\n","\n","    plt.subplot(1,2,2)\n","    plt.plot(shash_incs,shash_cpd,\n","             linewidth=5,\n","             color=clr_choice,\n","            )\n","\n","    k = np.argmin(np.abs(shash_perc_low[-1]-shash_incs))\n","    plt.plot((shash_perc_low[-1],shash_perc_low[-1]),(0,shash_cpd[k]),'--',color=clr_choice)\n","    k = np.argmin(np.abs(shash_perc_high[-1]-shash_incs))\n","    plt.plot((shash_perc_high[-1],shash_perc_high[-1]),(0,shash_cpd[k]),'--',color=clr_choice)\n","\n","    plt.xlabel('years until target')\n","    plt.title('Predictions for Observations\\nYear = 2021')\n","    plt.xlim(-10,40)\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"a9c4a925-e736-44cd-9629-b1623a04d5bb","metadata":{"id":"a9c4a925-e736-44cd-9629-b1623a04d5bb"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"_train_model_MAIN-Copy1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}