{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a650402-4774-49cb-9b72-9c8f1dd02f1d",
   "metadata": {
    "id": "4a650402-4774-49cb-9b72-9c8f1dd02f1d"
   },
   "source": [
    "# Detecting temperature targets\n",
    "[![Latest release](https://badgen.net/github/release/Naereen/Strapdown.js)](https://github.com/eabarnes1010/target_temp_detection/tree/main)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eabarnes1010/target_temp_detection/blob/main/_train_model.ipynb)\n",
    "\n",
    "_(ignore the badges for now - they are for when I post on github)_\n",
    "##### authors: Elizabeth A. Barnes and Noah Diffenbaugh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482fc82c-3e30-4466-b896-1b292d5b2ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Python setup stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811e52c-15c4-4e8a-8cb9-da43cb4ec4ee",
   "metadata": {
    "id": "c811e52c-15c4-4e8a-8cb9-da43cb4ec4ee",
    "tags": []
   },
   "source": [
    "The following code is for setting up a local environment.\n",
    "```\n",
    "conda create --name env-noah python=3.9\n",
    "conda activate env-noah\n",
    "pip install tensorflow==2.7.0\n",
    "pip install tensorflow-probability==0.15.0\n",
    "pip install --upgrade numpy scipy pandas statsmodels matplotlib seaborn palettable progressbar2 tabulate icecream flake8 keras-tuner sklearn jupyterlab black isort jupyterlab_code_formatter\n",
    "pip install -U scikit-learn\n",
    "pip install silence-tensorflow tqdm\n",
    "conda install -c conda-forge cmocean cartopy\n",
    "conda install -c conda-forge xarray dask netCDF4 bottleneck\n",
    "conda install -c conda-forge nc-time-axis\n",
    "```\n",
    "\n",
    "Use the command\n",
    "```python -m pip freeze > requirements.txt```\n",
    "to make a pip installation list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc0268-dd0a-4296-8a0b-f14b4c6189b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1646449663380,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "1fdc0268-dd0a-4296-8a0b-f14b4c6189b2",
    "outputId": "4e614eaa-b436-4f26-f1bd-3b59fcad893e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "print('IN_COLAB = ' + str(IN_COLAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb968382-4186-466e-a85b-b00caa5fc9be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17642,
     "status": "ok",
     "timestamp": 1646449680995,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "fb968382-4186-466e-a85b-b00caa5fc9be",
    "outputId": "d7964af9-2d52-4466-902d-9b85faba9a91",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install xarray==0.20.2\n",
    "    !pip install nc-time-axis\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Softmax\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "mpl.rcParams[\"figure.dpi\"] = 150\n",
    "savefig_dpi = 300\n",
    "np.warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5cee3-6f4f-4818-92e1-1351eeeb565a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1646449681009,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "29a5cee3-6f4f-4818-92e1-1351eeeb565a",
    "outputId": "e5f5b0ac-82b8-4147-bf44-4bc3b49466a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"python version = {sys.version}\")\n",
    "print(f\"numpy version = {np.__version__}\")\n",
    "print(f\"xarray version = {xr.__version__}\")  \n",
    "print(f\"tensorflow version = {tf.__version__}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de167a6-1c3f-435f-815c-0ff078ef9d1e",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0a166-fe1b-4040-a2f0-5b637089087a",
   "metadata": {
    "id": "2ca0a166-fe1b-4040-a2f0-5b637089087a"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f7ba0-d553-48c6-8e9e-106aa1b01279",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "executionInfo": {
     "elapsed": 20818,
     "status": "ok",
     "timestamp": 1646449701798,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "3d5f7ba0-d553-48c6-8e9e-106aa1b01279",
    "outputId": "d9357538-9525-4ba2-ad3c-530852889368"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install wget  \n",
    "    import wget\n",
    "    nc_filename = wget.download('https://eabarnes-data.atmos.colostate.edu/share/b.e21.BHISTsmbb-BSSP370smbb.f09_g17.LE2.cam.h0.TREFHT.185001-210012.r144x72.annual.nc')\n",
    "else:\n",
    "    DATA_DIRECTORY = 'data/'\n",
    "    nc_filename = DATA_DIRECTORY + 'b.e21.BHISTsmbb-BSSP370smbb.f09_g17.LE2.cam.h0.TREFHT.185001-210012.r144x72.annual.nc'\n",
    "\n",
    "da = xr.open_dataarray(nc_filename) - 273.15\n",
    "# da = da[:,da['time.year']>=1960,:,:]\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa439aca-8552-4f9e-8897-cbee5652a669",
   "metadata": {
    "id": "fa439aca-8552-4f9e-8897-cbee5652a669",
    "tags": []
   },
   "source": [
    "## Set training / validation / testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6afadf4-a952-4efc-b668-42721cc3c048",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1646449701798,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "a6afadf4-a952-4efc-b668-42721cc3c048",
    "outputId": "f9b25a39-804a-4400-e593-693f01d37b65"
   },
   "outputs": [],
   "source": [
    "TRAIN_MEMBERS = np.arange(0,27)\n",
    "VAL_MEMBERS   = np.arange(27,30)\n",
    "TEST_MEMBERS  = np.arange(27,30)\n",
    "print(TRAIN_MEMBERS, VAL_MEMBERS, TEST_MEMBERS)\n",
    "\n",
    "data_train = da[TRAIN_MEMBERS,:,:,:]\n",
    "data_val   = da[VAL_MEMBERS,:,:,:]\n",
    "data_test  = da[TEST_MEMBERS,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53d22e-95cb-4a2e-86b7-05dd68e5136e",
   "metadata": {
    "id": "ca53d22e-95cb-4a2e-86b7-05dd68e5136e"
   },
   "source": [
    "## Define the target year and labels\n",
    "\n",
    "For starters, we will define the target year as the year that the ensemble mean, global mean temperature crosses the ```TARGET_TEMP``` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae33333-64de-4bad-86c8-73b31083dacb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "executionInfo": {
     "elapsed": 2814,
     "status": "ok",
     "timestamp": 1646449704589,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "aae33333-64de-4bad-86c8-73b31083dacb",
    "outputId": "195a23ad-e907-478f-d6bf-758b287e3068"
   },
   "outputs": [],
   "source": [
    "# compute the ensemble mean, global mean temperature\n",
    "# these computations should be based on the training set only\n",
    "da_ens = data_train.mean(axis=0)\n",
    "weights = np.cos(np.deg2rad(da_ens.lat))\n",
    "weights.name = \"weights\"\n",
    "temp_weighted = da_ens.weighted(weights)\n",
    "global_mean = temp_weighted.mean((\"lon\", \"lat\"))\n",
    "\n",
    "# plot the calculation to make sure things make sense\n",
    "global_mean.plot(linewidth=2,label='data',color=\"tab:blue\")\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('temp (C)')\n",
    "plt.title('ensemble mean, global mean temperature')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa7117-3850-4e92-96d4-fe7a52541599",
   "metadata": {},
   "source": [
    "#### Define the labels for each sample as the number of years until the target is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd7c0d-1d1f-45b7-ada4-864d4863d22d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1646449704707,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "f3fd7c0d-1d1f-45b7-ada4-864d4863d22d",
    "outputId": "ed46afe3-80ad-47d5-f3fa-49ccf0e86ffc"
   },
   "outputs": [],
   "source": [
    "# define the labels\n",
    "labels = global_mean.values\n",
    "print('labels = ' + str(labels))\n",
    "\n",
    "# get years for each label/sample\n",
    "years = data_val['time.year'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739e225-d312-4a59-b6e6-71e04b38bd5b",
   "metadata": {},
   "source": [
    "#### Organize the data into training / validation / testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb22ef-5374-41ba-a198-bc84ec5abb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train.values.reshape((data_train.shape[0]*data_train.shape[1],data_train.shape[2]*data_train.shape[3]))\n",
    "x_val   = data_val.values.reshape((data_val.shape[0]*data_val.shape[1],data_val.shape[2]*data_val.shape[3]))\n",
    "x_test  = data_test.values.reshape((data_test.shape[0]*data_test.shape[1],data_test.shape[2]*data_test.shape[3]))\n",
    "\n",
    "y_train = np.tile(labels,data_train.shape[0])\n",
    "y_val   = np.tile(labels,data_val.shape[0])\n",
    "y_test  = np.tile(labels,data_test.shape[0])\n",
    "\n",
    "years_train = np.tile(years,data_train.shape[0])\n",
    "years_val   = np.tile(years,data_val.shape[0])\n",
    "years_test  = np.tile(years,data_test.shape[0])\n",
    "\n",
    "print(x_train.shape, y_train.shape, years_test.shape)\n",
    "print(x_val.shape, y_val.shape, years_test.shape)\n",
    "print(x_test.shape, y_test.shape, years_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf95abe-3856-45d6-9204-6cb9bad3dfaf",
   "metadata": {
    "id": "6bf95abe-3856-45d6-9204-6cb9bad3dfaf"
   },
   "source": [
    "## Setup the network and associated callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffe81f-e03b-4fd1-bf6a-8afbfeedce72",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1646449704707,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "7dffe81f-e03b-4fd1-bf6a-8afbfeedce72"
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the model and its compile function\n",
    "def compile_model(x_train):\n",
    "\n",
    "    # First we start with an input layer\n",
    "    inputs = Input(shape=x_train.shape[1:]) \n",
    "\n",
    "    normalizer = tf.keras.layers.Normalization()\n",
    "    normalizer.adapt(x_train)\n",
    "    layers = normalizer(inputs)\n",
    "\n",
    "    layers = Dropout(rate=DROP_RATE,\n",
    "                     seed=SEED)(layers) \n",
    "    \n",
    "    for hidden, activation, ridge in zip(HIDDENS, ACTIVATIONS, RIDGE):\n",
    "        layers = Dense(hidden, activation=activation,\n",
    "                       kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.00, l2=ridge),\n",
    "                       bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED),\n",
    "                       kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED))(layers)\n",
    "\n",
    "\n",
    "    # Output layer has a softmax function to convert output to class likelihood\n",
    "    output_layer = Dense(1, activation='linear',\n",
    "                      bias_initializer=tf.keras.initializers.RandomNormal(seed=SEED),\n",
    "                      kernel_initializer=tf.keras.initializers.RandomNormal(seed=SEED))(layers)\n",
    "\n",
    "    # Constructing the model\n",
    "    model = Model(inputs, output_layer)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) # Using the Adam optimizer\n",
    "    model.compile(optimizer=optimizer, loss=LOSS, metrics=['mse',])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c807abd7-832a-484b-98cd-7e6c3a9f60c0",
   "metadata": {
    "id": "c807abd7-832a-484b-98cd-7e6c3a9f60c0"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becb266-c9fd-4098-a2ba-e6c52804b8bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "executionInfo": {
     "elapsed": 105064,
     "status": "ok",
     "timestamp": 1646449809976,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "7becb266-c9fd-4098-a2ba-e6c52804b8bd",
    "outputId": "5f2d4b54-fb88-418f-95a2-3c5e281cc2e4"
   },
   "outputs": [],
   "source": [
    "SEED = 8889\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10_000\n",
    "HIDDENS = [5,5]\n",
    "RIDGE = [.75, 0.]\n",
    "DROP_RATE = 0.0\n",
    "ACTIVATIONS = ['relu'] * len(HIDDENS) \n",
    "LOSS = 'mae'\n",
    "PATIENCE = 30\n",
    "VERBOSITY = 0\n",
    "#----------------------------------------\n",
    "\n",
    "tf.keras.backend.clear_session()            \n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# define early stopping callback\n",
    "EARLY_STOPPING = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                   patience=PATIENCE,\n",
    "                                                   verbose=1,\n",
    "                                                   mode='auto',\n",
    "                                                   restore_best_weights=True)\n",
    "\n",
    "model = compile_model(x_train)\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=NUM_EPOCHS, \n",
    "                    verbose=VERBOSITY,\n",
    "                    batch_size = BATCH_SIZE, \n",
    "                    shuffle=True,\n",
    "                    validation_data=[x_val, y_val],\n",
    "                    callbacks=[EARLY_STOPPING],\n",
    "                   )\n",
    "#----------------------------------------\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b05d0-da60-4eda-86b7-4327333093c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1646449810953,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiNPVVIWP6XAkP_hwu-8rAxoeeNuk2BMkX5-yuA=s64",
      "userId": "07585723222468022011"
     },
     "user_tz": 420
    },
    "id": "156b05d0-da60-4eda-86b7-4327333093c1",
    "outputId": "6d37c46a-c8a3-4e44-df23-edd7e9b38697"
   },
   "outputs": [],
   "source": [
    "x_plot     = x_val #x_test #x_val\n",
    "y_plot     = y_val #y_test #y_val\n",
    "years_plot = years_val\n",
    "\n",
    "predict_plot = model.predict(x_plot)\n",
    "mae = np.mean(np.abs(predict_plot[:,0]-y_plot[:]))\n",
    "\n",
    "#--------------------------------\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(y_plot, predict_plot,'.')\n",
    "plt.plot(y_plot, y_plot, '--', color='fuchsia')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('actual ensemble-mean temperatue (C)')\n",
    "plt.ylabel('predicted temperature (C)')\n",
    "plt.title('Validation Data\\n MAE = ' + str(np.round(mae,2)) + ' C')\n",
    "\n",
    "#--------------------------------\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(years_plot, predict_plot,'.')\n",
    "plt.plot(global_mean['time.year'].values, global_mean.values, color='fuchsia', linewidth=2)\n",
    "plt.xlabel('year of map')\n",
    "plt.ylabel('predicted temperature (C)')\n",
    "\n",
    "if IN_COLAB==False:\n",
    "    pass\n",
    "    # plt.savefig('figures/initial_result_seed' + str(SEED) + '.png', dpi=savefig_dpi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ea61d-e514-415e-be5a-15b43367134f",
   "metadata": {},
   "source": [
    "## Explainability via Input * Gradient and Integrated Gradients\n",
    "We will use two attribution explainaiblity methods called Input * Gradient and Integrated Gradients to make heatmaps of regions of the input that act as explanations for the network's prediction.\n",
    "\n",
    "* https://keras.io/examples/vision/integrated_gradients/\n",
    "* https://distill.pub/2020/attribution-baselines/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364bda5b-6cc2-4117-8820-44cdf07b4c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gradients(inputs, top_pred_idx=None):\n",
    "    \"\"\"Computes the gradients of outputs w.r.t input image.\n",
    "\n",
    "    Args:\n",
    "        inputs: 2D/3D/4D matrix of samples\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.\n",
    "\n",
    "    Returns:\n",
    "        Gradients of the predictions w.r.t img_input\n",
    "    \"\"\"\n",
    "    inputs = tf.cast(inputs, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)\n",
    "        \n",
    "        # Run the forward pass of the layer and record operations\n",
    "        # on GradientTape.\n",
    "        preds = model(inputs, training=False)  \n",
    "        \n",
    "        # For classification, grab the top class\n",
    "        if top_pred_idx is not None:\n",
    "            preds = preds[:, top_pred_idx]\n",
    "        \n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.        \n",
    "    grads = tape.gradient(preds, inputs)\n",
    "    return grads\n",
    "\n",
    "def get_integrated_gradients(inputs, baseline=None, num_steps=50, top_pred_idx=None):\n",
    "    \"\"\"Computes Integrated Gradients for a prediction.\n",
    "\n",
    "    Args:\n",
    "        inputs (ndarray): 2D/3D/4D matrix of samples\n",
    "        baseline (ndarray): The baseline image to start with for interpolation\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.            \n",
    "\n",
    "    Returns:\n",
    "        Integrated gradients w.r.t input image\n",
    "    \"\"\"\n",
    "    # If baseline is not provided, start with zeros\n",
    "    # having same size as the input image.\n",
    "    if baseline is None:\n",
    "        input_size = np.shape(inputs)[1:]\n",
    "        baseline = np.zeros(input_size).astype(np.float32)\n",
    "    else:\n",
    "        baseline = baseline.astype(np.float32)\n",
    "\n",
    "    # 1. Do interpolation.\n",
    "    inputs = inputs.astype(np.float32)\n",
    "    interpolated_inputs = [\n",
    "        baseline + (step / num_steps) * (inputs - baseline)\n",
    "        for step in range(num_steps + 1)\n",
    "    ]\n",
    "    interpolated_inputs = np.array(interpolated_inputs).astype(np.float32)\n",
    "\n",
    "    # 3. Get the gradients\n",
    "    grads = []\n",
    "    for i, x_data in enumerate(interpolated_inputs):\n",
    "        grad = get_gradients(x_data, top_pred_idx=top_pred_idx)\n",
    "        grads.append(grad[0])\n",
    "    grads = tf.convert_to_tensor(grads, dtype=tf.float32)\n",
    "\n",
    "    # 4. Approximate the integral using the trapezoidal rule\n",
    "    grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "    avg_grads = tf.reduce_mean(grads, axis=0)\n",
    "\n",
    "    # 5. Calculate integrated gradients and return\n",
    "    integrated_grads = (inputs - baseline) * avg_grads\n",
    "    return integrated_grads\n",
    "\n",
    "def random_baseline_integrated_gradients(inputs, num_steps=50, num_runs=5, top_pred_idx=None):\n",
    "    \"\"\"Generates a number of random baseline images.\n",
    "\n",
    "    Args:\n",
    "        inputs (ndarray): 2D/3D/4D matrix of samples\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "        num_runs: number of baseline images to generate\n",
    "        top_pred_idx: (optional) Predicted label for the x_data\n",
    "                      if classification problem. If regression,\n",
    "                      do not include.      \n",
    "\n",
    "    Returns:\n",
    "        Averaged integrated gradients for `num_runs` baseline images\n",
    "    \"\"\"\n",
    "    # 1. List to keep track of Integrated Gradients (IG) for all the images\n",
    "    integrated_grads = []\n",
    "\n",
    "    # 2. Get the integrated gradients for all the baselines\n",
    "    for run in range(num_runs):\n",
    "        baseline = np.zeros(np.shape(inputs)[1:])\n",
    "        for i in np.arange(0,np.shape(random_baseline)[0]):\n",
    "            j = np.random.choice(np.arange(0,np.shape(inputs)[0]))\n",
    "            baseline[i] = inputs[j,i]\n",
    "\n",
    "        igrads = get_integrated_gradients(\n",
    "            inputs=inputs,\n",
    "            baseline=baseline,\n",
    "            num_steps=num_steps,\n",
    "        )\n",
    "        integrated_grads.append(igrads)\n",
    "\n",
    "    # 3. Return the average integrated gradients for the image\n",
    "    integrated_grads = tf.convert_to_tensor(integrated_grads)\n",
    "    return tf.reduce_mean(integrated_grads, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef431b-27ad-4874-959e-169e6f7f198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================\n",
    "# Define the samples you want to explain\n",
    "inputs = np.copy(x_val)\n",
    "targets = np.copy(y_val)\n",
    "years = np.copy(years_val)\n",
    "preds = model.predict(inputs)\n",
    "#========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc382a-b5ce-4fca-a9a3-4cdae743276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------\n",
    "# Gradient x Input\n",
    "#---------------------------------------\n",
    "# compute the multiplication of gradient * inputs\n",
    "# and reshape into a map of latitude x longitude\n",
    "grads = get_gradients(inputs).numpy()\n",
    "grad_x_input = grads * inputs\n",
    "grad_x_input = grad_x_input.reshape((len(targets),data_train.shape[2],data_train.shape[3]))\n",
    "print(np.shape(grad_x_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4cc98-6135-4fd8-9673-8f8af6a6e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------\n",
    "# Integrated Gradients\n",
    "#---------------------------------------\n",
    "BASELINE = 'zeros'#1850\n",
    "\n",
    "# Alternative baselines\n",
    "if(BASELINE=='zeros'):\n",
    "    baseline_mean = baseline_mean*0.    \n",
    "elif(BASELINE=='mean'):\n",
    "    baseline_mean = np.mean(x_train,axis=0)  \n",
    "else:\n",
    "    baseline_mean = np.mean(data_train,axis=0)\n",
    "    isample = np.where(data_train['time.year']==BASELINE)[0]\n",
    "    baseline_mean = np.mean(baseline_mean[isample,:,:],axis=0).values\n",
    "    baseline_mean = baseline_mean.reshape((baseline_mean.shape[0]*baseline_mean.shape[1]))\n",
    "\n",
    "print('shape(baseline_mean) = ' + str(np.shape(baseline_mean)))\n",
    "print('model.predict(baseline_mean) = ' + str(model.predict(baseline_mean[np.newaxis,:])))\n",
    "\n",
    "igrad = get_integrated_gradients(inputs, baseline=baseline_mean)\n",
    "integrated_gradients = igrad.numpy().reshape((len(targets),data_train.shape[2],data_train.shape[3]))\n",
    "print(np.shape(integrated_gradients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c536cb-85e5-4b49-84ab-7de73aa82a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(x, clim=None, title=None, text=None, cmap='RdGy_r'):\n",
    "    if clim is None:\n",
    "        plt.pcolor(x,\n",
    "                   cmap=cmap,\n",
    "                   norm=mpl.colors.CenteredNorm(),        \n",
    "                  )\n",
    "    else:\n",
    "        plt.pcolor(x,\n",
    "                   cmap=cmap,\n",
    "                  )\n",
    "    plt.clim(clim)\n",
    "    plt.colorbar()\n",
    "    plt.title(title,fontsize=15,loc='right')    \n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.text(0.01, 1.0, text, fontfamily='monospace', fontsize='small', va='bottom',transform=plt.gca().transAxes)\n",
    "\n",
    "# plot the resulting heatmaps for a subset of samples\n",
    "# based on their label\n",
    "plot_list = (1850, 1900, 1950, 2000, 2050)\n",
    "NCOL = 4\n",
    "plt.subplots(len(plot_list),NCOL,figsize=(35,5*len(plot_list)))\n",
    "\n",
    "for irow,min_range in enumerate(plot_list):\n",
    "        \n",
    "    max_range = min_range + 5\n",
    "    isamples = np.where((years >= min_range) & (years <= max_range))[0]\n",
    "    igrad_mean = np.mean(integrated_gradients[isamples,:,:],axis=0)\n",
    "    grad_x_input_mean = np.mean(grad_x_input[isamples,:,:],axis=0)\n",
    "    grad_mean = np.mean(grads[isamples,:],axis=0).reshape((data_train.shape[2],data_train.shape[3]))\n",
    "    x_inputs_mean = np.mean(inputs[isamples,:],axis=0).reshape((data_train.shape[2],data_train.shape[3]))\n",
    "    x_inputs_mean = x_inputs_mean - baseline_mean.reshape((data_train.shape[2],data_train.shape[3]))\n",
    "    #------------------------------------------------------------------\n",
    "    \n",
    "    text = (\n",
    "            \"\\n\"\n",
    "            + f\"  approx_labels  = {targets[isamples].mean().round(2)}\\n\"\n",
    "            + f\"  year_range     = {min_range}-{max_range}\\n\"        \n",
    "            + f\"  baseline       = {BASELINE}\\n\"                \n",
    "            + f\"  n_samples      = {len(isamples)}\\n\"\n",
    "    )    \n",
    "    #------------------------------------------------------------------    \n",
    "    \n",
    "    # plot average input map\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+1)\n",
    "    plot_map(x_inputs_mean, \n",
    "             text=text,\n",
    "             clim=(-10,10),\n",
    "             cmap='RdBu_r',\n",
    "             title = 'Temperature anomaly from Baseline',\n",
    "            )\n",
    "    #------------------------------------------------------------------\n",
    "    # plot explainability of gradient (saliency)\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+2)\n",
    "    plot_map(grad_mean, \n",
    "             text=text,             \n",
    "             clim=(-0.008, .008), \n",
    "             title = 'Gradient (Saliency)',\n",
    "            )\n",
    "    \n",
    "    #------------------------------------------------------------------\n",
    "    # plot explainability of input x gradient\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+3)\n",
    "    plot_map(grad_x_input_mean, \n",
    "             text=text,\n",
    "             clim=(-.1,.1),\n",
    "             title = 'Gradient x Input',\n",
    "            )\n",
    "\n",
    "    #------------------------------------------------------------------\n",
    "    # plot explainability of integrated gradients\n",
    "    plt.subplot(len(plot_list),NCOL,irow*NCOL+4)\n",
    "    plot_map(igrad_mean, \n",
    "             text=text,             \n",
    "             clim=(-.01,.01), \n",
    "             title = 'Integrated Gradients',\n",
    "            )\n",
    "\n",
    "plt.tight_layout()   \n",
    "if IN_COLAB==False:\n",
    "    plt.savefig('figures/xai_grid_' + str(min_range) +'-' + str(max_range) + '_baseline_' + str(BASELINE) + '.png', dpi=savefig_dpi)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe28ad8-862a-401c-97c1-f8eb44f05038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "_main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
